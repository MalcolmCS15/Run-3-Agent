{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0bb4b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful for Run3 project!\n",
      "TensorFlow version: 2.15.0\n",
      "NumPy version: 1.26.4\n",
      "OpenCV version: 4.10.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# -------- Image processing -----\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mss\n",
    "\n",
    "# -------- TensorFlow / Keras ----\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# -------- Misc / Debug ----------\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "print(\"All imports successful for Run3 project!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3e43e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish some variables and params for later\n",
    "\n",
    "#number of frames to feed model at a time. We input to the model the FRAME_STACK most recent frames\n",
    "FRAME_STACK = 4\n",
    "\n",
    "#variables for the location of the run 3 game on the screen. This is for Malcolms computer, if its diff for u make new vars\n",
    "TOP_X = 275\n",
    "TOP_Y = 195\n",
    "WIDTH = 725\n",
    "HEIGHT = 545\n",
    "\n",
    "GAMEOVER_X = 860\n",
    "GAMEOVER_Y = 435\n",
    "GAMEOVER_W = 70\n",
    "GAMEOVER_H = 45\n",
    "\n",
    "RUNWAY_X = 600\n",
    "RUNWAY_Y = 480\n",
    "RUNWAY_W = 75\n",
    "RUNWAY_H = 180\n",
    "\n",
    "RUNWAY_REWARD = 1\n",
    "\n",
    "#Which device is running the game. Add ur own if u wanna train. So we dont have to go all the way through everything and change\n",
    "MAC_LAPTOP = True \n",
    "MAC_MONITOR = False\n",
    "\n",
    "if MAC_LAPTOP: \n",
    "    TOP_X = 275\n",
    "    TOP_Y = 195\n",
    "    WIDTH = 725\n",
    "    HEIGHT = 545\n",
    "\n",
    "    GAMEOVER_X = 860\n",
    "    GAMEOVER_Y = 435\n",
    "    GAMEOVER_W = 70\n",
    "    GAMEOVER_H = 45\n",
    "\n",
    "    RUNWAY_X = 600\n",
    "    RUNWAY_Y = 480\n",
    "    RUNWAY_W = 75\n",
    "    RUNWAY_H = 180 #made this a bit longer to try and get the agent to move over the platforms when its mid-jump\n",
    "\n",
    "\n",
    "#resolution of the image were resizing to. This affects the input to our neural net directly.\n",
    "RESOLUTION = 128\n",
    "\n",
    "#number of actions the model can take. This is a super important thing to change if the model isnt training well. As of 12/5 were starting\n",
    "#with the model being able to take [no action, L_small, R_small, U_small, L_med, R_med ...etc.]\n",
    "NUM_ACTIONS = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(image, resize=(RESOLUTION, RESOLUTION), gray=True):\n",
    "    \"\"\"\n",
    "    Captures a screenshot of the given region, converts to grayscale, resizes.\n",
    "    Returns numpy array of shape (resize[1], resize[0]).\n",
    "    \"\"\"\n",
    "    if gray:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize deterministically\n",
    "    small = cv2.resize(image, resize, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return small\n",
    "\n",
    "def stack_frames(frames, new_frame, stack_size=FRAME_STACK):\n",
    "    \"\"\"\n",
    "    Maintains a stack of frames to capture motion.\n",
    "    frames: deque of previous frames\n",
    "    new_frame: newest preprocessed frame\n",
    "    Returns stack of frames\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        # Initialize with repeated frame\n",
    "        for _ in range(stack_size):\n",
    "            frames.append(new_frame)\n",
    "    else:\n",
    "        frames.append(new_frame)\n",
    "        if len(frames) > stack_size:\n",
    "            frames.popleft()\n",
    "    return np.stack(frames, axis=0)\n",
    "\n",
    "#TEST to see if screen grab is working\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "sct = mss.mss()\n",
    "monitor = {\n",
    "    \"top\": TOP_Y,\n",
    "    \"left\": TOP_X,\n",
    "    \"width\": WIDTH,\n",
    "    \"height\": HEIGHT\n",
    "}\n",
    "\n",
    "runway = {\n",
    "    \"top\": RUNWAY_Y,\n",
    "    \"left\": RUNWAY_X,\n",
    "    \"width\": RUNWAY_W,\n",
    "    \"height\": RUNWAY_H\n",
    "}\n",
    "screenshot = np.array(sct.grab(monitor))\n",
    "img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_BGRA2BGR)\n",
    "processed = preprocess_frame(img)\n",
    "plt.imshow(processed, cmap=\"gray\")\n",
    "plt.title(\"Preprocessed frame (grayscale + resized)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "screenshot = np.array(sct.grab(runway))\n",
    "img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_BGRA2BGR)\n",
    "processed = preprocess_frame(img)\n",
    "plt.imshow(processed, cmap=\"gray\")\n",
    "plt.title(\"Runway preview\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b158ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorCritic(tf.keras.Model):\n",
    "    def __init__(self, input_channels=FRAME_STACK, num_actions=NUM_ACTIONS):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "\n",
    "        # TensorFlow expects channels-last → (RESOLUTION, RESOLUTION, C)\n",
    "        self.input_channels = input_channels\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # ---------- CNN Backbone ----------\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=4, strides=2, activation='relu')\n",
    "        self.conv3 = layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')\n",
    "\n",
    "        #max pool? think about max pool if we use a larger resolution. But these convs also scale down.\n",
    "\n",
    "        # compute flatten size\n",
    "        self._conv_out_size = self._get_conv_out((RESOLUTION, RESOLUTION, input_channels))\n",
    "\n",
    "        # ---------- Shared Fully Connected ----------\n",
    "        self.fc = layers.Dense(512, activation='relu')\n",
    "\n",
    "        # ---------- Actor Head ----------\n",
    "        self.actor_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.actor_logits = layers.Dense(num_actions, activation=None)\n",
    "\n",
    "        # ---------- Critic Head ----------\n",
    "        self.critic_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.critic_value = layers.Dense(1, activation=None)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Compute conv output size by running dummy tensor. This saves us work if we change the CNN structure\n",
    "    # -------------------------------------------------\n",
    "    def _get_conv_out(self, shape):\n",
    "        dummy = tf.zeros((1, *shape), dtype=tf.float32)\n",
    "        x = self.conv1(dummy)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.shape[1:]))\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Forward pass\n",
    "    # -------------------------------------------------\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x expected as (batch, RESOLUTION, RESOLUTION, 4)\n",
    "        \"\"\"\n",
    "        x = tf.cast(x, tf.float32) / 255.0 #normalize to [0,1]\n",
    "\n",
    "        # CNN backbone\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = tf.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "        # Shared FC\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # ---- Actor ----\n",
    "        a = self.actor_fc1(x)\n",
    "        logits = self.actor_logits(a)\n",
    "\n",
    "        # ---- Critic ----\n",
    "        c = self.critic_fc1(x)\n",
    "        value = self.critic_value(c)\n",
    "\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "026a525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run3Env:\n",
    "    def __init__(self, region=(TOP_X, TOP_Y, WIDTH, HEIGHT), frame_stack=FRAME_STACK):\n",
    "        self.region = region\n",
    "        self.frame_stack = frame_stack #integer, not the actual stack. Do we need instance var?\n",
    "        self.frames = deque(maxlen=frame_stack)\n",
    "\n",
    "        self.sct = mss.mss()\n",
    "        self.monitor = {\n",
    "            \"top\": region[1],      # TOP_Y\n",
    "            \"left\": region[0],     # TOP_X\n",
    "            \"width\": region[2],    # WIDTH\n",
    "            \"height\": region[3]    # HEIGHT\n",
    "        }\n",
    "\n",
    "    # -------------------------\n",
    "    # Reset environment\n",
    "    # -------------------------\n",
    "    def reset(self):\n",
    "        # Click to restart game. 900,650 is just off the screen a bit, click twice to bypass the \"continue\" and \"score\". \n",
    "        #we can also press a button to make it better for everyones computer\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        self.frames.clear()\n",
    "\n",
    "        # Get initial observation\n",
    "        raw = self.capture_raw()\n",
    "        processed = preprocess_frame(raw) #function defined at the top\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        return np.transpose(stacked, (1, 2, 0))  # (RESOLUTION, RESOLUTION, FRAME_STACK). \n",
    "\n",
    "    # -------------------------\n",
    "    # Capture raw screenshot\n",
    "    # -------------------------\n",
    "    def capture_raw(self):\n",
    "        screenshot = np.array(self.sct.grab(self.monitor))\n",
    "        # mss returns BGRA, convert to BGR. Also mss is much faster than pyautogui so we use it for more fps.\n",
    "        img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "        return img\n",
    "\n",
    "    # -------------------------\n",
    "    # Detect game over\n",
    "    # -------------------------\n",
    "    def game_over(self, raw_frame): #FIX\n",
    "        \"\"\"Check if dialog region is white\"\"\"\n",
    "        # Extract region. Note hard coded values are for macs laptop, its a region of the screen where its all white on game over.\n",
    "        tlx = GAMEOVER_X - TOP_X\n",
    "        tly = GAMEOVER_Y - TOP_Y\n",
    "        w = GAMEOVER_W\n",
    "        h = GAMEOVER_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "        \n",
    "        # Check if white directly on BGR image\n",
    "        mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "        \n",
    "        # If all channels are ~255, mean will be ~255\n",
    "        return mean_val > 250\n",
    "\n",
    "    def runway_reward(self, raw_frame):\n",
    "        tlx = RUNWAY_X - TOP_X\n",
    "        tly = RUNWAY_Y - TOP_Y\n",
    "        w = RUNWAY_W\n",
    "        h = RUNWAY_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "        if len(roi.shape) == 3: #grayscale\n",
    "            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            roi_gray = roi\n",
    "        \n",
    "        platform_pixels = np.sum(roi_gray > 30)\n",
    "        total_pixels = roi_gray.size\n",
    "        platform_ratio = platform_pixels / total_pixels\n",
    "        \n",
    "        return platform_ratio * RUNWAY_REWARD #small reward based on % of runway occupied.\n",
    "\n",
    "    # -------------------------\n",
    "    # Take one step in environment \n",
    "    # -------------------------\n",
    "    def step(self, action): #NOT FINISHED DONT TOUCH\n",
    "        # step_start = time.time()\n",
    "        self._execute_action(action)\n",
    "\n",
    "        # Capture new frame\n",
    "        raw = self.capture_raw()\n",
    "\n",
    "        done = self.game_over(raw) #Boolean var\n",
    "\n",
    "        # Reward logic\n",
    "        if done:\n",
    "            reward = -5\n",
    "        else:\n",
    "            reward = 1 + self.runway_reward(raw) # small survival reward + a small alignment reward\n",
    "\n",
    "        # Preprocess\n",
    "        processed = preprocess_frame(raw)\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        \n",
    "        state = np.transpose(stacked, (1, 2, 0))  # (REOSLUTION,RESOLUTION,STACK_FRAMES)\n",
    "\n",
    "        # TARGET_TIMESTEP = 0.33 \n",
    "        # elapsed = time.time() - step_start\n",
    "        # if elapsed < TARGET_TIMESTEP:\n",
    "        #     time.sleep(TARGET_TIMESTEP - elapsed)\n",
    "\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _execute_action(self, action):\n",
    "        \"\"\"Execute action with proper hold durations\"\"\"\n",
    "    \n",
    "        # Map actions to (keys, duration_seconds)\n",
    "        # keys can be a tuple for combos\n",
    "        action_config = {\n",
    "            0: (None, 0),                # No action\n",
    "            1: (('left',), 0.05),\n",
    "            2: (('right',), 0.05),\n",
    "            3: (('up',), 0.05),\n",
    "            4: (('left',), 0.25),\n",
    "            5: (('right',), 0.25),\n",
    "            6: (('up',), 0.25),\n",
    "            7: (('left', 'up'), 0.05),\n",
    "            8: (('right', 'up'), 0.05),\n",
    "            9: (('left', 'up'), 0.25),\n",
    "            10: (('right', 'up'), 0.25),\n",
    "        }\n",
    "    \n",
    "        keys, duration = action_config[action]\n",
    "    \n",
    "        if keys is not None:\n",
    "            # Press down all keys in the combo\n",
    "            for k in keys:\n",
    "                pyautogui.keyDown(k)\n",
    "    \n",
    "            time.sleep(duration)\n",
    "    \n",
    "            # Release all keys\n",
    "            for k in keys:\n",
    "                pyautogui.keyUp(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4956b50f-186c-4637-9921-97c2687ae8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #JUST FOR TESTING THINGS WORK WHEN RUNNING THE GAME.\n",
    "# sct = mss.mss()\n",
    "# monitor = {\n",
    "#     \"top\": TOP_Y,\n",
    "#     \"left\": TOP_X,\n",
    "#     \"width\": WIDTH,\n",
    "#     \"height\": HEIGHT\n",
    "# }\n",
    "# num_g_over = 0\n",
    "\n",
    "# def test_game_over(raw_frame):\n",
    "#     \"\"\"Check if dialog region is white\"\"\"\n",
    "#     # Extract region\n",
    "#     tlx = 860 - TOP_X\n",
    "#     tly = 435 - TOP_Y\n",
    "#     w = 70\n",
    "#     h = 45\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "    \n",
    "#     # Check if white directly on BGR image\n",
    "#     mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "    \n",
    "#     # If all channels are ~255, mean will be ~255\n",
    "#     return mean_val > 250\n",
    "\n",
    "# def test_runway_reward(raw_frame):\n",
    "#     tlx = RUNWAY_X - TOP_X\n",
    "#     tly = RUNWAY_Y - TOP_Y\n",
    "#     w = RUNWAY_W\n",
    "#     h = RUNWAY_H\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "#     if len(roi.shape) == 3: #grayscale\n",
    "#         roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         roi_gray = roid\n",
    "    \n",
    "#     platform_pixels = np.sum(roi_gray > 30)\n",
    "#     total_pixels = roi_gray.size\n",
    "#     platform_ratio = platform_pixels / total_pixels\n",
    "    \n",
    "#     return platform_ratio\n",
    "\n",
    "# while True:\n",
    "#     start_time = time.time()\n",
    "#     screenshot = np.array(sct.grab(monitor))\n",
    "#     img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "#     if test_game_over(img):\n",
    "#         num_g_over += 1\n",
    "#         print(f\"\\rgame over {num_g_over}\", end='', flush=True)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#     else:\n",
    "#         pass\n",
    "#         # print(test_runway_reward(img))\n",
    "#     elapsed = time.time() - start_time\n",
    "#     print(elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3de65712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IHAVENT CHECKED THE CLASS BELOW YET THIS IS JUST CHAT SO DONT TREAT IT AS SOLIDLY IMPLEMENTED\n",
    "\n",
    "class PPOBuffer:\n",
    "    def __init__(self, size, obs_shape, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        size      : number of steps per rollout\n",
    "        obs_shape : shape of observation e.g. (RESOLUTION, RESOLUTION, FRAME_STACK)\n",
    "        gamma     : discount factor\n",
    "        lam       : GAE lambda\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "\n",
    "        # Allocate buffers\n",
    "        self.obs_buf = np.zeros((size, *obs_shape), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(size, dtype=np.int32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        # To be computed later\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0        # next index to write\n",
    "        self.path_start = 0 # start index of current trajectory\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Store one step of rollout data\n",
    "    # ---------------------------------------------------------\n",
    "    def store(self, obs, act, rew, done, val, logp):\n",
    "        assert self.ptr < self.size, \"PPOBuffer overflow!\"\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Finish trajectory and compute GAE + returns\n",
    "    # last_val is the value of the final observation (0 if done)\n",
    "    # ---------------------------------------------------------\n",
    "    def finish_trajectory(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Called at trajectory end or when episode completes.\n",
    "        Computes GAE advantage & discounted returns.\n",
    "        \"\"\"\n",
    "        i1 = self.path_start\n",
    "        i2 = self.ptr\n",
    "\n",
    "        rewards = np.append(self.rew_buf[i1:i2], last_val)\n",
    "        values  = np.append(self.val_buf[i1:i2], last_val)\n",
    "\n",
    "        # GAE-Lambda advantage calculation\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        adv = np.zeros_like(deltas)\n",
    "        last_gae = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            last_gae = deltas[t] + self.gamma * self.lam * last_gae * (1 - self.done_buf[i1 + t])\n",
    "            adv[t] = last_gae\n",
    "\n",
    "        self.adv_buf[i1:i2] = adv\n",
    "        self.ret_buf[i1:i2] = adv + self.val_buf[i1:i2]\n",
    "\n",
    "        self.path_start = self.ptr  # next trajectory starts here\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Retrieve all data, normalize adv, and reset pointer\n",
    "    # ---------------------------------------------------------\n",
    "    def prepare_for_training(self):\n",
    "        \"\"\"\n",
    "        Call this after all trajectories are collected and before get().\n",
    "        Normalizes advantages across the entire buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        # Normalize advantages\n",
    "        adv_mean = self.adv_buf.mean()\n",
    "        adv_std = self.adv_buf.std() + 1e-8\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "\n",
    "        \n",
    "    def get(self, batch_size=64):\n",
    "        \"\"\"Returns batches of rollout data.\"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        return {\n",
    "            'obs': self.obs_buf,\n",
    "            'act': self.act_buf,\n",
    "            'adv': self.adv_buf,\n",
    "            'ret': self.ret_buf,\n",
    "            'logp': self.logp_buf,\n",
    "            'val': self.val_buf\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset buffer for next rollout collection.\n",
    "        Call this after training is done for the epoch.\n",
    "        \"\"\"\n",
    "        self.ptr = 0\n",
    "        self.path_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4dcc7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning Rate: 0.0003\n",
      "  Steps per Epoch: 1024\n",
      "  Total Epochs: 5\n",
      "  Gamma: 0.99, GAE Lambda: 0.95\n",
      "  Clip Ratio: 0.2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION AND HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "CLIP_RATIO = 0.2\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.02\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# Rollout parameters\n",
    "STEPS_PER_EPOCH = 1024  # Number of steps per training epoch\n",
    "TRAIN_EPOCHS = 5      # Total number of epochs\n",
    "MINI_BATCH_SIZE = 64    # Size of mini-batches for SGD\n",
    "UPDATE_EPOCHS = 4      # Number of epochs to train on each batch\n",
    "\n",
    "# Discount and GAE\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "# Logging and checkpointing\n",
    "LOG_INTERVAL = 1          # Log every N epochs\n",
    "SAVE_INTERVAL = 15         # Save model every N epochs\n",
    "EVAL_EPISODES = 10          # Number of episodes for evaluation\n",
    "\n",
    "# Create directories for saving\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Steps per Epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"  Total Epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"  Gamma: {GAMMA}, GAE Lambda: {GAE_LAMBDA}\")\n",
    "print(f\"  Clip Ratio: {CLIP_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9d9d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PPO TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_loss(model, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Compute PPO loss with clipping.\n",
    "    \n",
    "    Args:\n",
    "        model: PPOActorCritic model\n",
    "        obs: observations (batch_size, H, W, C)\n",
    "        actions: actions taken (batch_size,)\n",
    "        advantages: advantage estimates (batch_size,)\n",
    "        returns: discounted returns (batch_size,)\n",
    "        old_log_probs: old action log probabilities (batch_size,)\n",
    "        old_values: old value estimates (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, policy_loss, value_loss, entropy\n",
    "    \"\"\"\n",
    "    # Get current policy and value predictions\n",
    "    logits, values = model(obs, training=True)\n",
    "    values = tf.squeeze(values, axis=-1)\n",
    "    \n",
    "    # Compute log probabilities of actions\n",
    "    action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "    log_probs = action_dist.log_prob(actions)\n",
    "    \n",
    "    # Compute entropy for exploration bonus\n",
    "    entropy = tf.reduce_mean(action_dist.entropy())\n",
    "    \n",
    "    # Compute ratio for PPO\n",
    "    ratio = tf.exp(log_probs - old_log_probs)\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
    "    \n",
    "    # Policy loss with clipping\n",
    "    policy_loss_1 = -advantages * ratio\n",
    "    policy_loss_2 = -advantages * tf.clip_by_value(ratio, 1 - CLIP_RATIO, 1 + CLIP_RATIO)\n",
    "    policy_loss = tf.reduce_mean(tf.maximum(policy_loss_1, policy_loss_2))\n",
    "    \n",
    "    # Value loss with clipping\n",
    "    value_pred_clipped = old_values + tf.clip_by_value(\n",
    "        values - old_values, -CLIP_RATIO, CLIP_RATIO\n",
    "    )\n",
    "    value_loss_1 = tf.square(returns - values)\n",
    "    value_loss_2 = tf.square(returns - value_pred_clipped)\n",
    "    value_loss = 0.5 * tf.reduce_mean(tf.maximum(value_loss_1, value_loss_2))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Single training step with gradient computation.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss, policy_loss, value_loss, entropy = compute_loss(\n",
    "            model, obs, actions, advantages, returns, old_log_probs, old_values\n",
    "        )\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    \n",
    "    # Clip gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, MAX_GRAD_NORM)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb306cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout collection function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA COLLECTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def collect_rollout(env, model, buffer, num_steps):\n",
    "    \"\"\"\n",
    "    Collect trajectories by running the current policy in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        buffer: PPOBuffer instance\n",
    "        num_steps: number of steps to collect\n",
    "    \n",
    "    Returns:\n",
    "        ep_returns: list of episode returns\n",
    "        ep_lengths: list of episode lengths\n",
    "    \"\"\"\n",
    "    buffer.reset()  # Clear the buffer\n",
    "    \n",
    "    ep_returns = []\n",
    "    ep_lengths = []\n",
    "    current_ep_return = 0\n",
    "    current_ep_length = 0\n",
    "    \n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Get action from policy\n",
    "        # Get action from policy\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)  # Add batch dimension\n",
    "        logits, value = model(obs_tensor, training=False)\n",
    "\n",
    "# Sample action from distribution (use logits, not probs!)\n",
    "        action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "        action_tensor = action_dist.sample()[0]\n",
    "        action = action_tensor.numpy()\n",
    "        log_prob = action_dist.log_prob(action_tensor).numpy()\n",
    "        value = value.numpy()[0, 0]\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        buffer.store(obs, action, reward, done, value, log_prob)\n",
    "        \n",
    "        current_ep_return += reward\n",
    "        current_ep_length += 1\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            # Episode finished\n",
    "            buffer.finish_trajectory(0)  # Terminal state has value 0\n",
    "            ep_returns.append(current_ep_return)\n",
    "            ep_lengths.append(current_ep_length)\n",
    "            \n",
    "            # Reset for next episode\n",
    "            obs = env.reset()\n",
    "            current_ep_return = 0\n",
    "            current_ep_length = 0\n",
    "    \n",
    "    # If we ended mid-episode, bootstrap the value\n",
    "    if current_ep_length > 0:\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "        _, last_value = model(obs_tensor, training=False)\n",
    "        buffer.finish_trajectory(last_value.numpy()[0, 0])\n",
    "        ep_returns.append(current_ep_return)\n",
    "        ep_lengths.append(current_ep_length)\n",
    "    \n",
    "    buffer.prepare_for_training()\n",
    "\n",
    "    return ep_returns, ep_lengths\n",
    "\n",
    "\n",
    "print(\"Rollout collection function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b8254b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_policy(env, model, num_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the current policy deterministically (greedy).\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        num_episodes: number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        mean_return: average episode return\n",
    "        std_return: standard deviation of returns\n",
    "        mean_length: average episode length\n",
    "    \"\"\"\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        ep_return = 0\n",
    "        ep_length = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Use greedy action (argmax)\n",
    "            obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "            logits, _ = model(obs_tensor, training=False)\n",
    "            action = tf.argmax(logits[0]).numpy()\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_return += reward\n",
    "            ep_length += 1\n",
    "            \n",
    "            # Safety: break if episode too long\n",
    "            if ep_length > 10000:\n",
    "                break\n",
    "        \n",
    "        episode_returns.append(ep_return)\n",
    "        episode_lengths.append(ep_length)\n",
    "    \n",
    "    return (\n",
    "        np.mean(episode_returns),\n",
    "        np.std(episode_returns),\n",
    "        np.mean(episode_lengths)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "344df394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training components...\n",
      "Model initialized with 4,863,857 parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-fog-5</strong> at: <a href='https://wandb.ai/Run-3/Run-3/runs/bcztkocj' target=\"_blank\">https://wandb.ai/Run-3/Run-3/runs/bcztkocj</a><br> View project at: <a href='https://wandb.ai/Run-3/Run-3' target=\"_blank\">https://wandb.ai/Run-3/Run-3</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251208_154421-bcztkocj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Mac/Desktop/csci1470/final-run3/wandb/run-20251208_155735-0tn1mrjt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Run-3/Run-3/runs/0tn1mrjt' target=\"_blank\">copper-donkey-6</a></strong> to <a href='https://wandb.ai/Run-3/Run-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Run-3/Run-3' target=\"_blank\">https://wandb.ai/Run-3/Run-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Run-3/Run-3/runs/0tn1mrjt' target=\"_blank\">https://wandb.ai/Run-3/Run-3/runs/0tn1mrjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Initializing training components...\")\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "# Training metrics\n",
    "training_stats = {\n",
    "    'epoch': [],\n",
    "    'mean_return': [],\n",
    "    'mean_length': [],\n",
    "    'policy_loss': [],\n",
    "    'value_loss': [],\n",
    "    'entropy': [],\n",
    "    'eval_return': [],\n",
    "    'eval_std': []\n",
    "}\n",
    "\n",
    "#set up weights and biases tracking\n",
    "run = wandb.init(\n",
    "    entity=\"Run-3\",\n",
    "    project=\"Run-3\",\n",
    "    config={\n",
    "        # Learning parameters\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"clip_ratio\": CLIP_RATIO,\n",
    "        \"value_coef\": VALUE_COEF,\n",
    "        \"entropy_coef\": ENTROPY_COEF,\n",
    "        \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "        \n",
    "        # Rollout parameters\n",
    "        \"steps_per_epoch\": STEPS_PER_EPOCH,\n",
    "        \"train_epochs\": TRAIN_EPOCHS,\n",
    "        \"mini_batch_size\": MINI_BATCH_SIZE,\n",
    "        \"update_epochs\": UPDATE_EPOCHS,\n",
    "        \n",
    "        # Discount and GAE\n",
    "        \"gamma\": GAMMA,\n",
    "        \"gae_lambda\": GAE_LAMBDA,\n",
    "        \n",
    "        # Environment\n",
    "        \"resolution\": RESOLUTION,\n",
    "        \"frame_stack\": FRAME_STACK,\n",
    "        \"num_actions\": NUM_ACTIONS,\n",
    "        \"runway_x\": RUNWAY_X,\n",
    "        \"runway_y\": RUNWAY_Y,\n",
    "        \"runway_w\": RUNWAY_W,\n",
    "        \"runway_h\": RUNWAY_H,\n",
    "        \"runway_reward\": RUNWAY_REWARD,\n",
    "        \n",
    "        # Model\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"num_parameters\": model.count_params(),\n",
    "        \n",
    "        # Evaluation\n",
    "        \"eval_episodes\": EVAL_EPISODES,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7c1bf418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r8/_jvmv7yj0jg5dzd73p_p2s2c0000gq/T/ipykernel_87906/1679292621.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.logp_buf[self.ptr] = logp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15 (378.7s)\n",
      "  Returns: 6.96 | Lengths: 8.4\n",
      "  Policy Loss: -0.0245\n",
      "  Value Loss: 18.5023\n",
      "  Entropy: 2.7674\n",
      "  Total Loss: 9.1712\n",
      "\n",
      "Epoch 2/15 (357.0s)\n",
      "  Returns: 11.53 | Lengths: 10.9\n",
      "  Policy Loss: -0.0199\n",
      "  Value Loss: 53.8504\n",
      "  Entropy: 2.7605\n",
      "  Total Loss: 26.8501\n",
      "\n",
      "Epoch 3/15 (341.9s)\n",
      "  Returns: 10.50 | Lengths: 10.2\n",
      "  Policy Loss: -0.0263\n",
      "  Value Loss: 38.5894\n",
      "  Entropy: 2.7378\n",
      "  Total Loss: 19.2137\n",
      "\n",
      "Epoch 4/15 (345.5s)\n",
      "  Returns: 7.18 | Lengths: 8.5\n",
      "  Policy Loss: -0.0174\n",
      "  Value Loss: 19.9399\n",
      "  Entropy: 2.6890\n",
      "  Total Loss: 9.8988\n",
      "\n",
      "Epoch 5/15 (340.4s)\n",
      "  Returns: 10.22 | Lengths: 10.2\n",
      "  Policy Loss: -0.0240\n",
      "  Value Loss: 34.7737\n",
      "  Entropy: 2.6985\n",
      "  Total Loss: 17.3089\n",
      "  [EVAL] Return: 17.57 ± 9.66 | Length: 15.6\n",
      "\n",
      "Epoch 6/15 (330.7s)\n",
      "  Returns: 12.90 | Lengths: 11.6\n",
      "  Policy Loss: -0.0247\n",
      "  Value Loss: 43.9134\n",
      "  Entropy: 2.7020\n",
      "  Total Loss: 21.8779\n",
      "\n",
      "Epoch 7/15 (344.2s)\n",
      "  Returns: 9.75 | Lengths: 10.0\n",
      "  Policy Loss: -0.0225\n",
      "  Value Loss: 29.4036\n",
      "  Entropy: 2.6715\n",
      "  Total Loss: 14.6258\n",
      "\n",
      "Epoch 8/15 (332.5s)\n",
      "  Returns: 13.20 | Lengths: 11.9\n",
      "  Policy Loss: -0.0238\n",
      "  Value Loss: 40.5958\n",
      "  Entropy: 2.6578\n",
      "  Total Loss: 20.2210\n",
      "\n",
      "Epoch 9/15 (349.5s)\n",
      "  Returns: 9.68 | Lengths: 9.8\n",
      "  Policy Loss: -0.0221\n",
      "  Value Loss: 23.7014\n",
      "  Entropy: 2.6397\n",
      "  Total Loss: 11.7758\n",
      "\n",
      "Epoch 10/15 (350.1s)\n",
      "  Returns: 10.50 | Lengths: 10.2\n",
      "  Policy Loss: -0.0268\n",
      "  Value Loss: 29.1310\n",
      "  Entropy: 2.6254\n",
      "  Total Loss: 14.4862\n",
      "  [EVAL] Return: 6.71 ± 6.81 | Length: 8.1\n",
      "\n",
      "Epoch 11/15 (354.6s)\n",
      "  Returns: 9.89 | Lengths: 10.0\n",
      "  Policy Loss: -0.0240\n",
      "  Value Loss: 20.4012\n",
      "  Entropy: 2.6476\n",
      "  Total Loss: 10.1237\n",
      "\n",
      "Epoch 12/15 (341.1s)\n",
      "  Returns: 10.86 | Lengths: 10.4\n",
      "  Policy Loss: -0.0231\n",
      "  Value Loss: 31.7057\n",
      "  Entropy: 2.6247\n",
      "  Total Loss: 15.7773\n",
      "\n",
      "Epoch 13/15 (343.3s)\n",
      "  Returns: 8.52 | Lengths: 9.3\n",
      "  Policy Loss: -0.0252\n",
      "  Value Loss: 14.6621\n",
      "  Entropy: 2.6083\n",
      "  Total Loss: 7.2537\n",
      "\n",
      "Epoch 14/15 (331.4s)\n",
      "  Returns: 11.95 | Lengths: 11.4\n",
      "  Policy Loss: -0.0245\n",
      "  Value Loss: 23.5536\n",
      "  Entropy: 2.5851\n",
      "  Total Loss: 11.7006\n",
      "\n",
      "Epoch 15/15 (319.3s)\n",
      "  Returns: 14.71 | Lengths: 12.8\n",
      "  Policy Loss: -0.0189\n",
      "  Value Loss: 38.1064\n",
      "  Entropy: 2.5544\n",
      "  Total Loss: 18.9832\n",
      "  [EVAL] Return: 25.18 ± 14.68 | Length: 20.5\n",
      "  [SAVED] Checkpoint: checkpoints/model_epoch_15.h5\n",
      "\n",
      "================================================================================\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # === 1. Collect rollout data ===\n",
    "    ep_returns, ep_lengths = collect_rollout(env, model, buffer, STEPS_PER_EPOCH)\n",
    "    \n",
    "    mean_return = np.mean(ep_returns) if len(ep_returns) > 0 else 0\n",
    "    mean_length = np.mean(ep_lengths) if len(ep_lengths) > 0 else 0\n",
    "    \n",
    "    # === 2. Get training data from buffer ===\n",
    "    data = buffer.get()\n",
    "    obs_buf = data['obs']\n",
    "    act_buf = data['act']\n",
    "    adv_buf = data['adv']\n",
    "    ret_buf = data['ret']\n",
    "    logp_buf = data['logp']\n",
    "    val_buf = data['val']\n",
    "    \n",
    "    # === 3. Train on the collected data ===\n",
    "    total_losses = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    # Perform multiple epochs of training on the same batch\n",
    "    for i in range(UPDATE_EPOCHS):\n",
    "        # Shuffle indices\n",
    "        indices = np.arange(len(obs_buf))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Train on mini-batches\n",
    "        for start in range(0, len(obs_buf), MINI_BATCH_SIZE):\n",
    "            end = start + MINI_BATCH_SIZE\n",
    "            batch_idx = indices[start:end]\n",
    "            \n",
    "            batch_obs = tf.constant(obs_buf[batch_idx])\n",
    "            batch_act = tf.constant(act_buf[batch_idx])\n",
    "            batch_adv = tf.constant(adv_buf[batch_idx])\n",
    "            batch_ret = tf.constant(ret_buf[batch_idx])\n",
    "            batch_logp = tf.constant(logp_buf[batch_idx])\n",
    "            batch_val = tf.constant(val_buf[batch_idx])\n",
    "            \n",
    "            # Perform gradient update\n",
    "            total_loss, policy_loss, value_loss, entropy = train_step(\n",
    "                model, optimizer, batch_obs, batch_act,\n",
    "                batch_adv, batch_ret, batch_logp, batch_val\n",
    "            )\n",
    "            \n",
    "            total_losses.append(total_loss.numpy())\n",
    "            policy_losses.append(policy_loss.numpy())\n",
    "            value_losses.append(value_loss.numpy())\n",
    "            entropies.append(entropy.numpy())\n",
    "    \n",
    "    # === 4. Logging ===\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    mean_policy_loss = np.mean(policy_losses)\n",
    "    mean_value_loss = np.mean(value_losses)\n",
    "    mean_entropy = np.mean(entropies)\n",
    "    mean_total_loss = np.mean(total_losses)\n",
    "\n",
    "    wandb.log({\n",
    "        \"train/mean_return\": mean_return,\n",
    "        \"train/mean_length\": mean_length,\n",
    "        \"train/policy_loss\": mean_policy_loss,\n",
    "        \"train/value_loss\": mean_value_loss,\n",
    "        \"train/entropy\": mean_entropy,\n",
    "        \"train/total_loss\": mean_total_loss,\n",
    "        \"train/epoch_time\": epoch_time,\n",
    "    })\n",
    "    \n",
    "    if (epoch + 1) % LOG_INTERVAL == 0:\n",
    "        print(f\"\\nEpoch {epoch + 1}/{TRAIN_EPOCHS} ({epoch_time:.1f}s)\")\n",
    "        print(f\"  Returns: {mean_return:.2f} | Lengths: {mean_length:.1f}\")\n",
    "        print(f\"  Policy Loss: {mean_policy_loss:.4f}\")\n",
    "        print(f\"  Value Loss: {mean_value_loss:.4f}\")\n",
    "        print(f\"  Entropy: {mean_entropy:.4f}\")\n",
    "        print(f\"  Total Loss: {mean_total_loss:.4f}\")\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if (epoch + 1) % (LOG_INTERVAL * 5) == 0:\n",
    "            eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "            print(f\"  [EVAL] Return: {eval_return:.2f} ± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "            training_stats['eval_return'].append(eval_return)\n",
    "            training_stats['eval_std'].append(eval_std)\n",
    "    \n",
    "    # Store metrics\n",
    "    training_stats['epoch'].append(epoch + 1)\n",
    "    training_stats['mean_return'].append(mean_return)\n",
    "    training_stats['mean_length'].append(mean_length)\n",
    "    training_stats['policy_loss'].append(mean_policy_loss)\n",
    "    training_stats['value_loss'].append(mean_value_loss)\n",
    "    training_stats['entropy'].append(mean_entropy)\n",
    "    \n",
    "    # === 5. Save checkpoint ===\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        checkpoint_path = f'checkpoints/model_epoch_{epoch + 1}.h5'\n",
    "        model.save_weights(checkpoint_path)\n",
    "        print(f\"  [SAVED] Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f2499d55-efb1-4ad2-bdbc-a662990bf2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 4,863,857 parameters\n",
      "Loaded model from checkpoints/model_epoch_15.h5\n",
      "  [EVAL] Return: 15.84 ± 13.48 | Length: 14.5\n"
     ]
    }
   ],
   "source": [
    "#JUST FOR LOADING AND EVALUATING A MODEL\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "model_path = 'checkpoints/model_epoch_15.h5'  # or whichever epoch you want\n",
    "model.load_weights(model_path)\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "print(f\"  [EVAL] Return: {eval_return:.2f} ± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "training_stats['eval_return'].append(eval_return)\n",
    "training_stats['eval_std'].append(eval_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78a597-272a-4951-8f32-aff17cf1cabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Run3 Project)",
   "language": "python",
   "name": "run3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
