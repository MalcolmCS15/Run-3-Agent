{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb4b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful for Run3 project!\n",
      "TensorFlow version: 2.15.0\n",
      "NumPy version: 1.26.4\n",
      "OpenCV version: 4.10.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# -------- Image processing -----\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mss\n",
    "\n",
    "# -------- TensorFlow / Keras ----\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# -------- Misc / Debug ----------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"All imports successful for Run3 project!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e43e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish some variables and params for later\n",
    "\n",
    "#number of frames to feed model at a time. We input to the model the FRAME_STACK most recent frames\n",
    "FRAME_STACK = 4\n",
    "\n",
    "#variables for the location of the run 3 game on the screen. This is for Malcolms computer, if its diff for u make new vars\n",
    "TOP_X = 275\n",
    "TOP_Y = 195\n",
    "WIDTH = 725\n",
    "HEIGHT = 545\n",
    "\n",
    "GAMEOVER_X = 860\n",
    "GAMEOVER_Y = 435\n",
    "GAMEOVER_W = 70\n",
    "GAMEOVER_H = 45\n",
    "\n",
    "RUNWAY_X = 600\n",
    "RUNWAY_Y = 480\n",
    "RUNWAY_W = 135\n",
    "RUNWAY_H = 230\n",
    "\n",
    "#Which device is running the game. Add ur own if u wanna train. So we dont have to go all the way through everything and change\n",
    "MAC_LAPTOP = True \n",
    "MAC_MONITOR = False\n",
    "\n",
    "if MAC_LAPTOP: \n",
    "    TOP_X = 275\n",
    "    TOP_Y = 195\n",
    "    WIDTH = 725\n",
    "    HEIGHT = 545\n",
    "\n",
    "    GAMEOVER_X = 860\n",
    "    GAMEOVER_Y = 435\n",
    "    GAMEOVER_W = 70\n",
    "    GAMEOVER_H = 45\n",
    "\n",
    "    RUNWAY_X = 600\n",
    "    RUNWAY_Y = 480\n",
    "    RUNWAY_W = 135\n",
    "    RUNWAY_H = 230\n",
    "\n",
    "\n",
    "#resolution of the image were resizing to. This affects the input to our neural net directly.\n",
    "RESOLUTION = 96\n",
    "\n",
    "#number of actions the model can take. This is a super important thing to change if the model isnt training well. As of 12/5 were starting\n",
    "#with the model being able to take [no action, L_small, R_small, U_small, L_med, R_med ...etc.]\n",
    "NUM_ACTIONS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e191780a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_frame(image, resize=(RESOLUTION, RESOLUTION), gray=True):\n",
    "    \"\"\"\n",
    "    Captures a screenshot of the given region, converts to grayscale, resizes.\n",
    "    Returns numpy array of shape (resize[1], resize[0]).\n",
    "    \"\"\"\n",
    "    if gray:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize deterministically\n",
    "    small = cv2.resize(image, resize, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return small\n",
    "\n",
    "def stack_frames(frames, new_frame, stack_size=FRAME_STACK):\n",
    "    \"\"\"\n",
    "    Maintains a stack of frames to capture motion.\n",
    "    frames: deque of previous frames\n",
    "    new_frame: newest preprocessed frame\n",
    "    Returns stack of frames\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        # Initialize with repeated frame\n",
    "        for _ in range(stack_size):\n",
    "            frames.append(new_frame)\n",
    "    else:\n",
    "        frames.append(new_frame)\n",
    "        if len(frames) > stack_size:\n",
    "            frames.popleft()\n",
    "    return np.stack(frames, axis=0)\n",
    "\n",
    "#TEST to see if screen grab is working\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "sct = mss.mss()\n",
    "monitor = {\n",
    "    \"top\": TOP_Y,\n",
    "    \"left\": TOP_X,\n",
    "    \"width\": WIDTH,\n",
    "    \"height\": HEIGHT\n",
    "}\n",
    "screenshot = np.array(sct.grab(monitor))\n",
    "img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_BGRA2BGR)\n",
    "processed = preprocess_frame(img)\n",
    "plt.imshow(processed, cmap=\"gray\")\n",
    "plt.title(\"Preprocessed frame (grayscale + resized)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b158ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorCritic(tf.keras.Model):\n",
    "    def __init__(self, input_channels=FRAME_STACK, num_actions=NUM_ACTIONS):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "\n",
    "        # TensorFlow expects channels-last â†’ (RESOLUTION, RESOLUTION, C)\n",
    "        self.input_channels = input_channels\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # ---------- CNN Backbone ----------\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=4, strides=2, activation='relu')\n",
    "        self.conv3 = layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')\n",
    "\n",
    "        #max pool? think about max pool if we use a larger resolution. But these convs also scale down.\n",
    "\n",
    "        # compute flatten size\n",
    "        self._conv_out_size = self._get_conv_out((RESOLUTION, RESOLUTION, input_channels))\n",
    "\n",
    "        # ---------- Shared Fully Connected ----------\n",
    "        self.fc = layers.Dense(512, activation='relu')\n",
    "\n",
    "        # ---------- Actor Head ----------\n",
    "        self.actor_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.actor_logits = layers.Dense(num_actions, activation=None)\n",
    "\n",
    "        # ---------- Critic Head ----------\n",
    "        self.critic_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.critic_value = layers.Dense(1, activation=None)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Compute conv output size by running dummy tensor. This saves us work if we change the CNN structure\n",
    "    # -------------------------------------------------\n",
    "    def _get_conv_out(self, shape):\n",
    "        dummy = tf.zeros((1, *shape), dtype=tf.float32)\n",
    "        x = self.conv1(dummy)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.shape[1:]))\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Forward pass\n",
    "    # -------------------------------------------------\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x expected as (batch, RESOLUTION, RESOLUTION, 4)\n",
    "        \"\"\"\n",
    "        x = tf.cast(x, tf.float32) / 255.0 #normalize to [0,1]\n",
    "\n",
    "        # CNN backbone\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = tf.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "        # Shared FC\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # ---- Actor ----\n",
    "        a = self.actor_fc1(x)\n",
    "        logits = self.actor_logits(a)\n",
    "\n",
    "        # ---- Critic ----\n",
    "        c = self.critic_fc1(x)\n",
    "        value = self.critic_value(c)\n",
    "\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "026a525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run3Env:\n",
    "    def __init__(self, region=(TOP_X, TOP_Y, WIDTH, HEIGHT), frame_stack=FRAME_STACK):\n",
    "        self.region = region\n",
    "        self.frame_stack = frame_stack #integer, not the actual stack. Do we need instance var?\n",
    "        self.frames = deque(maxlen=frame_stack)\n",
    "\n",
    "        self.sct = mss.mss()\n",
    "        self.monitor = {\n",
    "            \"top\": region[1],      # TOP_Y\n",
    "            \"left\": region[0],     # TOP_X\n",
    "            \"width\": region[2],    # WIDTH\n",
    "            \"height\": region[3]    # HEIGHT\n",
    "        }\n",
    "\n",
    "    # -------------------------\n",
    "    # Reset environment\n",
    "    # -------------------------\n",
    "    def reset(self):\n",
    "        # Click to restart game. 900,650 is just off the screen a bit, click twice to bypass the \"continue\" and \"score\". \n",
    "        #we can also press a button to make it better for everyones computer\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        self.frames.clear()\n",
    "\n",
    "        # Get initial observation\n",
    "        raw = self.capture_raw()\n",
    "        processed = preprocess_frame(raw) #function defined at the top\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        return np.transpose(stacked, (1, 2, 0))  # (RESOLUTION, RESOLUTION, FRAME_STACK). \n",
    "\n",
    "    # -------------------------\n",
    "    # Capture raw screenshot\n",
    "    # -------------------------\n",
    "    def capture_raw(self):\n",
    "        screenshot = np.array(self.sct.grab(self.monitor))\n",
    "        # mss returns BGRA, convert to BGR. Also mss is much faster than pyautogui so we use it for more fps.\n",
    "        img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "        return img\n",
    "\n",
    "    # -------------------------\n",
    "    # Detect game over\n",
    "    # -------------------------\n",
    "    def game_over(self, raw_frame): #FIX\n",
    "        \"\"\"Check if dialog region is white\"\"\"\n",
    "        # Extract region. Note hard coded values are for macs laptop, its a region of the screen where its all white on game over.\n",
    "        tlx = GAMEOVER_X - TOP_X\n",
    "        tly = GAMEOVER_Y - TOP_Y\n",
    "        w = GAMEOVER_W\n",
    "        h = GAMEOVER_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "        \n",
    "        # Check if white directly on BGR image\n",
    "        mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "        \n",
    "        # If all channels are ~255, mean will be ~255\n",
    "        return mean_val > 250\n",
    "\n",
    "    def runway_reward(self, raw_frame):\n",
    "        tlx = RUNWAY_X - TOP_X\n",
    "        tly = RUNWAY_Y - TOP_Y\n",
    "        w = RUNWAY_W\n",
    "        h = RUNWAY_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "        if len(roi.shape) == 3: #grayscale\n",
    "            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            roi_gray = roi\n",
    "        \n",
    "        platform_pixels = np.sum(roi_gray > 30)\n",
    "        total_pixels = roi_gray.size\n",
    "        platform_ratio = platform_pixels / total_pixels\n",
    "        \n",
    "        return platform_ratio #small reward based on % of runway occupied.\n",
    "\n",
    "    # -------------------------\n",
    "    # Take one step in environment \n",
    "    # -------------------------\n",
    "    def step(self, action): #NOT FINISHED DONT TOUCH\n",
    "        # step_start = time.time()\n",
    "        self._execute_action(action)\n",
    "\n",
    "        # Capture new frame\n",
    "        raw = self.capture_raw()\n",
    "\n",
    "        done = self.game_over(raw) #Boolean var\n",
    "\n",
    "        platform_score = self.runway_reward(raw)\n",
    "        # Reward logic\n",
    "        if done:\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward = 0.1 + 2.0 * platform_score # small survival reward + a small alignment reward\n",
    "            if action == 0:\n",
    "                reward -= 0.05\n",
    "        # Preprocess\n",
    "        processed = preprocess_frame(raw)\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        \n",
    "        state = np.transpose(stacked, (1, 2, 0))  # (REOSLUTION,RESOLUTION,STACK_FRAMES)\n",
    "\n",
    "        # TARGET_TIMESTEP = 0.33 \n",
    "        # elapsed = time.time() - step_start\n",
    "        # if elapsed < TARGET_TIMESTEP:\n",
    "        #     time.sleep(TARGET_TIMESTEP - elapsed)\n",
    "\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _execute_action(self, action):\n",
    "        \"\"\"Execute action with proper hold durations\"\"\"\n",
    "        # Map actions to (key, duration_seconds)\n",
    "        action_config = {\n",
    "            0: (None, 0),           # No action\n",
    "            1: ('left', 0.05),       # Left short - 100ms\n",
    "            2: ('right', 0.05),      # Right short\n",
    "            3: ('up', 0.05),         # Up short (jump)\n",
    "            4: ('left', 0.1),      # Left medium - 250ms\n",
    "            5: ('right', 0.1),     # Right medium\n",
    "            6: ('up', 0.1),        # Up medium\n",
    "            7: ('left', 0.25),       # Left long - 500ms\n",
    "            8: ('right', 0.25),      # Right long\n",
    "            9: ('up', 0.25),         # Up long\n",
    "        }\n",
    "        \n",
    "        key, duration = action_config[action]\n",
    "        \n",
    "        if key is not None:\n",
    "            pyautogui.keyDown(key)\n",
    "            time.sleep(duration)\n",
    "            pyautogui.keyUp(key)\n",
    "        time.sleep(0.25-duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4956b50f-186c-4637-9921-97c2687ae8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #JUST FOR TESTING THINGS WORK WHEN RUNNING THE GAME.\n",
    "# sct = mss.mss()\n",
    "# monitor = {\n",
    "#     \"top\": TOP_Y,\n",
    "#     \"left\": TOP_X,\n",
    "#     \"width\": WIDTH,\n",
    "#     \"height\": HEIGHT\n",
    "# }\n",
    "# num_g_over = 0\n",
    "\n",
    "# def test_game_over(raw_frame):\n",
    "#     \"\"\"Check if dialog region is white\"\"\"\n",
    "#     # Extract region\n",
    "#     tlx = 860 - TOP_X\n",
    "#     tly = 435 - TOP_Y\n",
    "#     w = 70\n",
    "#     h = 45\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "    \n",
    "#     # Check if white directly on BGR image\n",
    "#     mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "    \n",
    "#     # If all channels are ~255, mean will be ~255\n",
    "#     return mean_val > 250\n",
    "\n",
    "# def test_runway_reward(raw_frame):\n",
    "#     tlx = RUNWAY_X - TOP_X\n",
    "#     tly = RUNWAY_Y - TOP_Y\n",
    "#     w = RUNWAY_W\n",
    "#     h = RUNWAY_H\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "#     if len(roi.shape) == 3: #grayscale\n",
    "#         roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         roi_gray = roid\n",
    "    \n",
    "#     platform_pixels = np.sum(roi_gray > 30)\n",
    "#     total_pixels = roi_gray.size\n",
    "#     platform_ratio = platform_pixels / total_pixels\n",
    "    \n",
    "#     return platform_ratio\n",
    "\n",
    "# while True:\n",
    "#     start_time = time.time()\n",
    "#     screenshot = np.array(sct.grab(monitor))\n",
    "#     img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "#     if test_game_over(img):\n",
    "#         num_g_over += 1\n",
    "#         print(f\"\\rgame over {num_g_over}\", end='', flush=True)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#     else:\n",
    "#         pass\n",
    "#         # print(test_runway_reward(img))\n",
    "#     elapsed = time.time() - start_time\n",
    "#     if elapsed < 1:\n",
    "#             time.sleep(1 - elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3de65712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IHAVENT CHECKED THE CLASS BELOW YET THIS IS JUST CHAT SO DONT TREAT IT AS SOLIDLY IMPLEMENTED\n",
    "\n",
    "class PPOBuffer:\n",
    "    def __init__(self, size, obs_shape, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        size      : number of steps per rollout\n",
    "        obs_shape : shape of observation e.g. (RESOLUTION, RESOLUTION, FRAME_STACK)\n",
    "        gamma     : discount factor\n",
    "        lam       : GAE lambda\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "\n",
    "        # Allocate buffers\n",
    "        self.obs_buf = np.zeros((size, *obs_shape), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(size, dtype=np.int32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        # To be computed later\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0        # next index to write\n",
    "        self.path_start = 0 # start index of current trajectory\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Store one step of rollout data\n",
    "    # ---------------------------------------------------------\n",
    "    def store(self, obs, act, rew, done, val, logp):\n",
    "        assert self.ptr < self.size, \"PPOBuffer overflow!\"\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Finish trajectory and compute GAE + returns\n",
    "    # last_val is the value of the final observation (0 if done)\n",
    "    # ---------------------------------------------------------\n",
    "    def finish_trajectory(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Called at trajectory end or when episode completes.\n",
    "        Computes GAE advantage & discounted returns.\n",
    "        \"\"\"\n",
    "        i1 = self.path_start\n",
    "        i2 = self.ptr\n",
    "\n",
    "        rewards = np.append(self.rew_buf[i1:i2], last_val)\n",
    "        values  = np.append(self.val_buf[i1:i2], last_val)\n",
    "\n",
    "        # GAE-Lambda advantage calculation\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        adv = np.zeros_like(deltas)\n",
    "        last_gae = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            last_gae = deltas[t] + self.gamma * self.lam * last_gae * (1 - self.done_buf[i1 + t])\n",
    "            adv[t] = last_gae\n",
    "\n",
    "        self.adv_buf[i1:i2] = adv\n",
    "        self.ret_buf[i1:i2] = adv + self.val_buf[i1:i2]\n",
    "\n",
    "        self.path_start = self.ptr  # next trajectory starts here\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Retrieve all data, normalize adv, and reset pointer\n",
    "    # ---------------------------------------------------------\n",
    "    def prepare_for_training(self):\n",
    "        \"\"\"\n",
    "        Call this after all trajectories are collected and before get().\n",
    "        Normalizes advantages across the entire buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        # Normalize advantages\n",
    "        adv_mean = self.adv_buf.mean()\n",
    "        adv_std = self.adv_buf.std() + 1e-8\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "\n",
    "        \n",
    "    def get(self, batch_size=64):\n",
    "        \"\"\"Returns batches of rollout data.\"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        return {\n",
    "            'obs': self.obs_buf,\n",
    "            'act': self.act_buf,\n",
    "            'adv': self.adv_buf,\n",
    "            'ret': self.ret_buf,\n",
    "            'logp': self.logp_buf,\n",
    "            'val': self.val_buf\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset buffer for next rollout collection.\n",
    "        Call this after training is done for the epoch.\n",
    "        \"\"\"\n",
    "        self.ptr = 0\n",
    "        self.path_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dcc7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning Rate: 0.0003\n",
      "  Steps per Epoch: 512\n",
      "  Total Epochs: 150\n",
      "  Gamma: 0.99, GAE Lambda: 0.95\n",
      "  Clip Ratio: 0.2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION AND HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "CLIP_RATIO = 0.2\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.025\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# Rollout parameters\n",
    "STEPS_PER_EPOCH = 512  # Number of steps per training epoch\n",
    "TRAIN_EPOCHS = 150     # Total number of epochs\n",
    "MINI_BATCH_SIZE = 64    # Size of mini-batches for SGD\n",
    "UPDATE_EPOCHS = 5      # Number of epochs to train on each batch\n",
    "\n",
    "# Discount and GAE\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "# Logging and checkpointing\n",
    "LOG_INTERVAL = 3          # Log every N epochs\n",
    "SAVE_INTERVAL = 25         # Save model every N epochs\n",
    "EVAL_EPISODES = 10          # Number of episodes for evaluation\n",
    "\n",
    "# Create directories for saving\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Steps per Epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"  Total Epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"  Gamma: {GAMMA}, GAE Lambda: {GAE_LAMBDA}\")\n",
    "print(f\"  Clip Ratio: {CLIP_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9d9d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PPO TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_loss(model, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Compute PPO loss with clipping.\n",
    "    \n",
    "    Args:\n",
    "        model: PPOActorCritic model\n",
    "        obs: observations (batch_size, H, W, C)\n",
    "        actions: actions taken (batch_size,)\n",
    "        advantages: advantage estimates (batch_size,)\n",
    "        returns: discounted returns (batch_size,)\n",
    "        old_log_probs: old action log probabilities (batch_size,)\n",
    "        old_values: old value estimates (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, policy_loss, value_loss, entropy\n",
    "    \"\"\"\n",
    "    # Get current policy and value predictions\n",
    "    logits, values = model(obs, training=True)\n",
    "    values = tf.squeeze(values, axis=-1)\n",
    "    \n",
    "    # Compute log probabilities of actions\n",
    "    action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "    log_probs = action_dist.log_prob(actions)\n",
    "    \n",
    "    # Compute entropy for exploration bonus\n",
    "    entropy = tf.reduce_mean(action_dist.entropy())\n",
    "    \n",
    "    # Compute ratio for PPO\n",
    "    ratio = tf.exp(log_probs - old_log_probs)\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
    "    \n",
    "    # Policy loss with clipping\n",
    "    policy_loss_1 = -advantages * ratio\n",
    "    policy_loss_2 = -advantages * tf.clip_by_value(ratio, 1 - CLIP_RATIO, 1 + CLIP_RATIO)\n",
    "    policy_loss = tf.reduce_mean(tf.maximum(policy_loss_1, policy_loss_2))\n",
    "    \n",
    "    # Value loss with clipping\n",
    "    value_pred_clipped = old_values + tf.clip_by_value(\n",
    "        values - old_values, -CLIP_RATIO, CLIP_RATIO\n",
    "    )\n",
    "    value_loss_1 = tf.square(returns - values)\n",
    "    value_loss_2 = tf.square(returns - value_pred_clipped)\n",
    "    value_loss = 0.5 * tf.reduce_mean(tf.maximum(value_loss_1, value_loss_2))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Single training step with gradient computation.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss, policy_loss, value_loss, entropy = compute_loss(\n",
    "            model, obs, actions, advantages, returns, old_log_probs, old_values\n",
    "        )\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    \n",
    "    # Clip gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, MAX_GRAD_NORM)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb306cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout collection function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA COLLECTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def collect_rollout(env, model, buffer, num_steps):\n",
    "    \"\"\"\n",
    "    Collect trajectories by running the current policy in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        buffer: PPOBuffer instance\n",
    "        num_steps: number of steps to collect\n",
    "    \n",
    "    Returns:\n",
    "        ep_returns: list of episode returns\n",
    "        ep_lengths: list of episode lengths\n",
    "    \"\"\"\n",
    "    buffer.reset()  # Clear the buffer\n",
    "    \n",
    "    ep_returns = []\n",
    "    ep_lengths = []\n",
    "    current_ep_return = 0\n",
    "    current_ep_length = 0\n",
    "    \n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Get action from policy\n",
    "        # Get action from policy\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)  # Add batch dimension\n",
    "        logits, value = model(obs_tensor, training=False)\n",
    "\n",
    "# Sample action from distribution (use logits, not probs!)\n",
    "        action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "        action_tensor = action_dist.sample()[0]\n",
    "        action = action_tensor.numpy()\n",
    "        log_prob = action_dist.log_prob(action_tensor).numpy()\n",
    "        value = value.numpy()[0, 0]\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        buffer.store(obs, action, reward, done, value, log_prob)\n",
    "        \n",
    "        current_ep_return += reward\n",
    "        current_ep_length += 1\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            # Episode finished\n",
    "            buffer.finish_trajectory(0)  # Terminal state has value 0\n",
    "            ep_returns.append(current_ep_return)\n",
    "            ep_lengths.append(current_ep_length)\n",
    "            \n",
    "            # Reset for next episode\n",
    "            obs = env.reset()\n",
    "            current_ep_return = 0\n",
    "            current_ep_length = 0\n",
    "    \n",
    "    # If we ended mid-episode, bootstrap the value\n",
    "    if current_ep_length > 0:\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "        _, last_value = model(obs_tensor, training=False)\n",
    "        buffer.finish_trajectory(last_value.numpy()[0, 0])\n",
    "        ep_returns.append(current_ep_return)\n",
    "        ep_lengths.append(current_ep_length)\n",
    "    \n",
    "    buffer.prepare_for_training()\n",
    "\n",
    "    return ep_returns, ep_lengths\n",
    "\n",
    "\n",
    "print(\"Rollout collection function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b8254b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_policy(env, model, num_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the current policy deterministically (greedy).\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        num_episodes: number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        mean_return: average episode return\n",
    "        std_return: standard deviation of returns\n",
    "        mean_length: average episode length\n",
    "    \"\"\"\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        ep_return = 0\n",
    "        ep_length = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Use greedy action (argmax)\n",
    "            obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "            logits, _ = model(obs_tensor, training=False)\n",
    "            action = tf.argmax(logits[0]).numpy()\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_return += reward\n",
    "            ep_length += 1\n",
    "            \n",
    "            # Safety: break if episode too long\n",
    "            if ep_length > 10000:\n",
    "                break\n",
    "        \n",
    "        episode_returns.append(ep_return)\n",
    "        episode_lengths.append(ep_length)\n",
    "    \n",
    "    return (\n",
    "        np.mean(episode_returns),\n",
    "        np.std(episode_returns),\n",
    "        np.mean(episode_lengths)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344df394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training components...\n",
      "Model initialized with 2,242,027 parameters\n",
      "\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Initializing training components...\")\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "# Training metrics\n",
    "training_stats = {\n",
    "    'epoch': [],\n",
    "    'mean_return': [],\n",
    "    'mean_length': [],\n",
    "    'policy_loss': [],\n",
    "    'value_loss': [],\n",
    "    'entropy': [],\n",
    "    'eval_return': [],\n",
    "    'eval_std': []\n",
    "}\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c1bf418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/q_/y2tlnq4d3gl5gqxkc15y__ym0000gn/T/ipykernel_55550/1310795699.py:36: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/q_/y2tlnq4d3gl5gqxkc15y__ym0000gn/T/ipykernel_55550/1310795699.py:36: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/ops/distributions/categorical.py:230: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/ops/distributions/categorical.py:230: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "/var/folders/q_/y2tlnq4d3gl5gqxkc15y__ym0000gn/T/ipykernel_55550/1679292621.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.logp_buf[self.ptr] = logp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/150 (343.6s)\n",
      "  Returns: 1.04 | Lengths: 9.3\n",
      "  Policy Loss: -0.0182\n",
      "  Value Loss: 21.7675\n",
      "  Entropy: 2.2538\n",
      "\n",
      "Epoch 6/150 (349.5s)\n",
      "  Returns: 0.20 | Lengths: 9.0\n",
      "  Policy Loss: -0.0271\n",
      "  Value Loss: 16.6255\n",
      "  Entropy: 2.2088\n",
      "\n",
      "Epoch 9/150 (325.4s)\n",
      "  Returns: 3.88 | Lengths: 11.1\n",
      "  Policy Loss: -0.0279\n",
      "  Value Loss: 27.8666\n",
      "  Entropy: 2.0575\n",
      "\n",
      "Epoch 12/150 (323.8s)\n",
      "  Returns: 7.01 | Lengths: 12.5\n",
      "  Policy Loss: -0.0175\n",
      "  Value Loss: 34.1855\n",
      "  Entropy: 2.1061\n",
      "\n",
      "Epoch 15/150 (330.8s)\n",
      "  Returns: 2.98 | Lengths: 10.4\n",
      "  Policy Loss: -0.0282\n",
      "  Value Loss: 20.7703\n",
      "  Entropy: 2.0067\n",
      "  [EVAL] Return: 7.24 Â± 8.22 | Length: 12.6\n",
      "\n",
      "Epoch 18/150 (310.2s)\n",
      "  Returns: 4.97 | Lengths: 11.6\n",
      "  Policy Loss: -0.0195\n",
      "  Value Loss: 47.4705\n",
      "  Entropy: 1.8683\n",
      "\n",
      "Epoch 21/150 (308.7s)\n",
      "  Returns: 3.98 | Lengths: 10.9\n",
      "  Policy Loss: -0.0165\n",
      "  Value Loss: 28.7975\n",
      "  Entropy: 1.6717\n",
      "\n",
      "Epoch 24/150 (291.2s)\n",
      "  Returns: 6.59 | Lengths: 12.8\n",
      "  Policy Loss: -0.0206\n",
      "  Value Loss: 31.4076\n",
      "  Entropy: 1.5656\n",
      "  [SAVED] Checkpoint: checkpoints/model_epoch_25.h5\n",
      "\n",
      "Epoch 27/150 (271.6s)\n",
      "  Returns: 11.90 | Lengths: 16.0\n",
      "  Policy Loss: -0.0139\n",
      "  Value Loss: 70.7678\n",
      "  Entropy: 1.3492\n",
      "\n",
      "Epoch 30/150 (272.6s)\n",
      "  Returns: 7.82 | Lengths: 13.1\n",
      "  Policy Loss: -0.0165\n",
      "  Value Loss: 43.4716\n",
      "  Entropy: 1.2163\n",
      "  [EVAL] Return: 40.03 Â± 107.02 | Length: 30.5\n",
      "\n",
      "Epoch 33/150 (283.3s)\n",
      "  Returns: 7.06 | Lengths: 12.8\n",
      "  Policy Loss: -0.0090\n",
      "  Value Loss: 24.5103\n",
      "  Entropy: 1.3671\n",
      "\n",
      "Epoch 36/150 (287.5s)\n",
      "  Returns: 7.09 | Lengths: 12.5\n",
      "  Policy Loss: -0.0197\n",
      "  Value Loss: 31.0670\n",
      "  Entropy: 1.4166\n",
      "\n",
      "Epoch 39/150 (227.0s)\n",
      "  Returns: 28.64 | Lengths: 24.4\n",
      "  Policy Loss: -0.0160\n",
      "  Value Loss: 164.4602\n",
      "  Entropy: 0.7887\n",
      "\n",
      "Epoch 42/150 (157.7s)\n",
      "  Returns: 983.12 | Lengths: 512.0\n",
      "  Policy Loss: 0.0276\n",
      "  Value Loss: 397.6926\n",
      "  Entropy: 0.5015\n",
      "\n",
      "Epoch 45/150 (322.1s)\n",
      "  Returns: 3.73 | Lengths: 10.9\n",
      "  Policy Loss: -0.0102\n",
      "  Value Loss: 28.7687\n",
      "  Entropy: 1.9505\n",
      "  [EVAL] Return: 3.19 Â± 8.12 | Length: 10.9\n",
      "\n",
      "Epoch 48/150 (332.6s)\n",
      "  Returns: 2.26 | Lengths: 10.0\n",
      "  Policy Loss: -0.0324\n",
      "  Value Loss: 26.2271\n",
      "  Entropy: 1.8835\n",
      "  [SAVED] Checkpoint: checkpoints/model_epoch_50.h5\n",
      "\n",
      "Epoch 51/150 (315.8s)\n",
      "  Returns: 5.34 | Lengths: 11.9\n",
      "  Policy Loss: -0.0208\n",
      "  Value Loss: 28.5453\n",
      "  Entropy: 1.7842\n",
      "\n",
      "Epoch 54/150 (319.4s)\n",
      "  Returns: 3.89 | Lengths: 11.6\n",
      "  Policy Loss: -0.0304\n",
      "  Value Loss: 16.1956\n",
      "  Entropy: 1.7348\n",
      "\n",
      "Epoch 57/150 (309.4s)\n",
      "  Returns: 8.23 | Lengths: 13.5\n",
      "  Policy Loss: -0.0109\n",
      "  Value Loss: 45.0193\n",
      "  Entropy: 1.6124\n",
      "\n",
      "Epoch 60/150 (314.2s)\n",
      "  Returns: 5.84 | Lengths: 11.9\n",
      "  Policy Loss: -0.0217\n",
      "  Value Loss: 23.7118\n",
      "  Entropy: 1.5460\n",
      "  [EVAL] Return: 17.45 Â± 30.50 | Length: 20.6\n",
      "\n",
      "Epoch 63/150 (304.1s)\n",
      "  Returns: 6.27 | Lengths: 12.5\n",
      "  Policy Loss: -0.0094\n",
      "  Value Loss: 16.6289\n",
      "  Entropy: 1.5309\n",
      "\n",
      "Epoch 66/150 (301.4s)\n",
      "  Returns: 7.72 | Lengths: 13.8\n",
      "  Policy Loss: -0.0276\n",
      "  Value Loss: 35.5637\n",
      "  Entropy: 1.5437\n",
      "\n",
      "Epoch 69/150 (295.7s)\n",
      "  Returns: 10.81 | Lengths: 15.5\n",
      "  Policy Loss: -0.0270\n",
      "  Value Loss: 46.4503\n",
      "  Entropy: 1.4970\n",
      "\n",
      "Epoch 72/150 (283.5s)\n",
      "  Returns: 12.34 | Lengths: 15.5\n",
      "  Policy Loss: -0.0104\n",
      "  Value Loss: 66.9201\n",
      "  Entropy: 1.3670\n",
      "\n",
      "Epoch 75/150 (319.7s)\n",
      "  Returns: 4.30 | Lengths: 11.6\n",
      "  Policy Loss: -0.0374\n",
      "  Value Loss: 20.5663\n",
      "  Entropy: 1.6478\n",
      "  [EVAL] Return: 40.30 Â± 38.05 | Length: 30.9\n",
      "  [SAVED] Checkpoint: checkpoints/model_epoch_75.h5\n",
      "\n",
      "Epoch 78/150 (300.7s)\n",
      "  Returns: 4.00 | Lengths: 11.9\n",
      "  Policy Loss: -0.0281\n",
      "  Value Loss: 19.3462\n",
      "  Entropy: 1.4838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m epoch_start_time = time.time()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# === 1. Collect rollout data ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ep_returns, ep_lengths = \u001b[43mcollect_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m mean_return = np.mean(ep_returns) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ep_returns) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     12\u001b[39m mean_length = np.mean(ep_lengths) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ep_lengths) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mcollect_rollout\u001b[39m\u001b[34m(env, model, buffer, num_steps)\u001b[39m\n\u001b[32m     40\u001b[39m value = value.numpy()[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Take action in environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m next_obs, reward, done, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Store transition\u001b[39;00m\n\u001b[32m     46\u001b[39m buffer.store(obs, action, reward, done, value, log_prob)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mRun3Env.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action): \u001b[38;5;66;03m#NOT FINISHED DONT TOUCH\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# step_start = time.time()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# Capture new frame\u001b[39;00m\n\u001b[32m     86\u001b[39m     raw = \u001b[38;5;28mself\u001b[39m.capture_raw()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mRun3Env._execute_action\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    131\u001b[39m     time.sleep(duration)\n\u001b[32m    132\u001b[39m     pyautogui.keyUp(key)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m time.sleep(\u001b[32m0.25\u001b[39m-duration)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # === 1. Collect rollout data ===\n",
    "    ep_returns, ep_lengths = collect_rollout(env, model, buffer, STEPS_PER_EPOCH)\n",
    "    \n",
    "    mean_return = np.mean(ep_returns) if len(ep_returns) > 0 else 0\n",
    "    mean_length = np.mean(ep_lengths) if len(ep_lengths) > 0 else 0\n",
    "    \n",
    "    # === 2. Get training data from buffer ===\n",
    "    data = buffer.get()\n",
    "    obs_buf = data['obs']\n",
    "    act_buf = data['act']\n",
    "    adv_buf = data['adv']\n",
    "    ret_buf = data['ret']\n",
    "    logp_buf = data['logp']\n",
    "    val_buf = data['val']\n",
    "    \n",
    "    # === 3. Train on the collected data ===\n",
    "    total_losses = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    # Perform multiple epochs of training on the same batch\n",
    "    for i in range(UPDATE_EPOCHS):\n",
    "        # Shuffle indices\n",
    "        indices = np.arange(len(obs_buf))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Train on mini-batches\n",
    "        for start in range(0, len(obs_buf), MINI_BATCH_SIZE):\n",
    "            end = start + MINI_BATCH_SIZE\n",
    "            batch_idx = indices[start:end]\n",
    "            \n",
    "            batch_obs = tf.constant(obs_buf[batch_idx])\n",
    "            batch_act = tf.constant(act_buf[batch_idx])\n",
    "            batch_adv = tf.constant(adv_buf[batch_idx])\n",
    "            batch_ret = tf.constant(ret_buf[batch_idx])\n",
    "            batch_logp = tf.constant(logp_buf[batch_idx])\n",
    "            batch_val = tf.constant(val_buf[batch_idx])\n",
    "            \n",
    "            # Perform gradient update\n",
    "            total_loss, policy_loss, value_loss, entropy = train_step(\n",
    "                model, optimizer, batch_obs, batch_act,\n",
    "                batch_adv, batch_ret, batch_logp, batch_val\n",
    "            )\n",
    "            \n",
    "            total_losses.append(total_loss.numpy())\n",
    "            policy_losses.append(policy_loss.numpy())\n",
    "            value_losses.append(value_loss.numpy())\n",
    "            entropies.append(entropy.numpy())\n",
    "    \n",
    "    # === 4. Logging ===\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    if (epoch + 1) % LOG_INTERVAL == 0:\n",
    "        print(f\"\\nEpoch {epoch + 1}/{TRAIN_EPOCHS} ({epoch_time:.1f}s)\")\n",
    "        print(f\"  Returns: {mean_return:.2f} | Lengths: {mean_length:.1f}\")\n",
    "        print(f\"  Policy Loss: {np.mean(policy_losses):.4f}\")\n",
    "        print(f\"  Value Loss: {np.mean(value_losses):.4f}\")\n",
    "        print(f\"  Entropy: {np.mean(entropies):.4f}\")\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if (epoch + 1) % (LOG_INTERVAL * 5) == 0:\n",
    "            eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "            print(f\"  [EVAL] Return: {eval_return:.2f} Â± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "            training_stats['eval_return'].append(eval_return)\n",
    "            training_stats['eval_std'].append(eval_std)\n",
    "    \n",
    "    # Store metrics\n",
    "    training_stats['epoch'].append(epoch + 1)\n",
    "    training_stats['mean_return'].append(mean_return)\n",
    "    training_stats['mean_length'].append(mean_length)\n",
    "    training_stats['policy_loss'].append(np.mean(policy_losses))\n",
    "    training_stats['value_loss'].append(np.mean(value_losses))\n",
    "    training_stats['entropy'].append(np.mean(entropies))\n",
    "    \n",
    "    # === 5. Save checkpoint ===\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        checkpoint_path = f'checkpoints/model_epoch_{epoch + 1}.h5'\n",
    "        model.save_weights(checkpoint_path)\n",
    "        print(f\"  [SAVED] Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2499d55-efb1-4ad2-bdbc-a662990bf2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST FOR LOADING AND EVALUATING A MODEL\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "model_path = 'checkpoints/model_epoch_75.h5'  # or whichever epoch you want\n",
    "model.load_weights(model_path)\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "print(f\"  [EVAL] Return: {eval_return:.2f} Â± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "training_stats['eval_return'].append(eval_return)\n",
    "training_stats['eval_std'].append(eval_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78a597-272a-4951-8f32-aff17cf1cabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
