{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb4b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful for Run3 project!\n",
      "TensorFlow version: 2.15.0\n",
      "NumPy version: 1.26.4\n",
      "OpenCV version: 4.10.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# -------- Image processing -----\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mss\n",
    "\n",
    "# -------- TensorFlow / Keras ----\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# -------- Misc / Debug ----------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"All imports successful for Run3 project!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e43e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish some variables and params for later\n",
    "\n",
    "#number of frames to feed model at a time. We input to the model the FRAME_STACK most recent frames\n",
    "FRAME_STACK = 4\n",
    "\n",
    "#variables for the location of the run 3 game on the screen. This is for Malcolms computer, if its diff for u make new vars\n",
    "TOP_X = 275\n",
    "TOP_Y = 195\n",
    "WIDTH = 725\n",
    "HEIGHT = 545\n",
    "\n",
    "GAMEOVER_X = 860\n",
    "GAMEOVER_Y = 435\n",
    "GAMEOVER_W = 70\n",
    "GAMEOVER_H = 45\n",
    "\n",
    "RUNWAY_X = 600\n",
    "RUNWAY_Y = 480\n",
    "RUNWAY_W = 135\n",
    "RUNWAY_H = 230\n",
    "\n",
    "#Which device is running the game. Add ur own if u wanna train. So we dont have to go all the way through everything and change\n",
    "MAC_LAPTOP = True \n",
    "MAC_MONITOR = False\n",
    "\n",
    "if MAC_LAPTOP: \n",
    "    TOP_X = 275\n",
    "    TOP_Y = 195\n",
    "    WIDTH = 725\n",
    "    HEIGHT = 545\n",
    "\n",
    "    GAMEOVER_X = 860\n",
    "    GAMEOVER_Y = 435\n",
    "    GAMEOVER_W = 70\n",
    "    GAMEOVER_H = 45\n",
    "\n",
    "    RUNWAY_X = 600\n",
    "    RUNWAY_Y = 480\n",
    "    RUNWAY_W = 135\n",
    "    RUNWAY_H = 230\n",
    "\n",
    "\n",
    "#resolution of the image were resizing to. This affects the input to our neural net directly.\n",
    "RESOLUTION = 96\n",
    "\n",
    "#number of actions the model can take. This is a super important thing to change if the model isnt training well. As of 12/5 were starting\n",
    "#with the model being able to take [no action, L_small, R_small, U_small, L_med, R_med ...etc.]\n",
    "NUM_ACTIONS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e191780a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RESOLUTION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_frame\u001b[39m(image, resize=(\u001b[43mRESOLUTION\u001b[49m, RESOLUTION), gray=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Captures a screenshot of the given region, converts to grayscale, resizes.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    Returns numpy array of shape (resize[1], resize[0]).\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gray:\n",
      "\u001b[31mNameError\u001b[39m: name 'RESOLUTION' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_frame(image, resize=(RESOLUTION, RESOLUTION), gray=True):\n",
    "    \"\"\"\n",
    "    Captures a screenshot of the given region, converts to grayscale, resizes.\n",
    "    Returns numpy array of shape (resize[1], resize[0]).\n",
    "    \"\"\"\n",
    "    if gray:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize deterministically\n",
    "    small = cv2.resize(image, resize, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return small\n",
    "\n",
    "def stack_frames(frames, new_frame, stack_size=FRAME_STACK):\n",
    "    \"\"\"\n",
    "    Maintains a stack of frames to capture motion.\n",
    "    frames: deque of previous frames\n",
    "    new_frame: newest preprocessed frame\n",
    "    Returns stack of frames\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        # Initialize with repeated frame\n",
    "        for _ in range(stack_size):\n",
    "            frames.append(new_frame)\n",
    "    else:\n",
    "        frames.append(new_frame)\n",
    "        if len(frames) > stack_size:\n",
    "            frames.popleft()\n",
    "    return np.stack(frames, axis=0)\n",
    "\n",
    "#TEST to see if screen grab is working\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "sct = mss.mss()\n",
    "monitor = {\n",
    "    \"top\": TOP_Y,\n",
    "    \"left\": TOP_X,\n",
    "    \"width\": WIDTH,\n",
    "    \"height\": HEIGHT\n",
    "}\n",
    "screenshot = np.array(sct.grab(monitor))\n",
    "img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_BGRA2BGR)\n",
    "processed = preprocess_frame(img)\n",
    "plt.imshow(processed, cmap=\"gray\")\n",
    "plt.title(\"Preprocessed frame (grayscale + resized)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b158ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorCritic(tf.keras.Model):\n",
    "    def __init__(self, input_channels=FRAME_STACK, num_actions=NUM_ACTIONS):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "\n",
    "        # TensorFlow expects channels-last → (RESOLUTION, RESOLUTION, C)\n",
    "        self.input_channels = input_channels\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # ---------- CNN Backbone ----------\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=4, strides=2, activation='relu')\n",
    "        self.conv3 = layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')\n",
    "\n",
    "        #max pool? think about max pool if we use a larger resolution. But these convs also scale down.\n",
    "\n",
    "        # compute flatten size\n",
    "        self._conv_out_size = self._get_conv_out((RESOLUTION, RESOLUTION, input_channels))\n",
    "\n",
    "        # ---------- Shared Fully Connected ----------\n",
    "        self.fc = layers.Dense(512, activation='relu')\n",
    "\n",
    "        # ---------- Actor Head ----------\n",
    "        self.actor_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.actor_logits = layers.Dense(num_actions, activation=None)\n",
    "\n",
    "        # ---------- Critic Head ----------\n",
    "        self.critic_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.critic_value = layers.Dense(1, activation=None)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Compute conv output size by running dummy tensor. This saves us work if we change the CNN structure\n",
    "    # -------------------------------------------------\n",
    "    def _get_conv_out(self, shape):\n",
    "        dummy = tf.zeros((1, *shape), dtype=tf.float32)\n",
    "        x = self.conv1(dummy)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.shape[1:]))\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Forward pass\n",
    "    # -------------------------------------------------\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x expected as (batch, RESOLUTION, RESOLUTION, 4)\n",
    "        \"\"\"\n",
    "        x = tf.cast(x, tf.float32) / 255.0 #normalize to [0,1]\n",
    "\n",
    "        # CNN backbone\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = tf.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "        # Shared FC\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # ---- Actor ----\n",
    "        a = self.actor_fc1(x)\n",
    "        logits = self.actor_logits(a)\n",
    "\n",
    "        # ---- Critic ----\n",
    "        c = self.critic_fc1(x)\n",
    "        value = self.critic_value(c)\n",
    "\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026a525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run3Env:\n",
    "    def __init__(self, region=(TOP_X, TOP_Y, WIDTH, HEIGHT), frame_stack=FRAME_STACK):\n",
    "        self.region = region\n",
    "        self.frame_stack = frame_stack #integer, not the actual stack. Do we need instance var?\n",
    "        self.frames = deque(maxlen=frame_stack)\n",
    "\n",
    "        self.sct = mss.mss()\n",
    "        self.monitor = {\n",
    "            \"top\": region[1],      # TOP_Y\n",
    "            \"left\": region[0],     # TOP_X\n",
    "            \"width\": region[2],    # WIDTH\n",
    "            \"height\": region[3]    # HEIGHT\n",
    "        }\n",
    "\n",
    "    # -------------------------\n",
    "    # Reset environment\n",
    "    # -------------------------\n",
    "    def reset(self):\n",
    "        # Click to restart game. 900,650 is just off the screen a bit, click twice to bypass the \"continue\" and \"score\". \n",
    "        #we can also press a button to make it better for everyones computer\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        self.frames.clear()\n",
    "\n",
    "        # Get initial observation\n",
    "        raw = self.capture_raw()\n",
    "        processed = preprocess_frame(raw) #function defined at the top\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        return np.transpose(stacked, (1, 2, 0))  # (RESOLUTION, RESOLUTION, FRAME_STACK). \n",
    "\n",
    "    # -------------------------\n",
    "    # Capture raw screenshot\n",
    "    # -------------------------\n",
    "    def capture_raw(self):\n",
    "        screenshot = np.array(self.sct.grab(self.monitor))\n",
    "        # mss returns BGRA, convert to BGR. Also mss is much faster than pyautogui so we use it for more fps.\n",
    "        img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "        return img\n",
    "\n",
    "    # -------------------------\n",
    "    # Detect game over\n",
    "    # -------------------------\n",
    "    def game_over(self, raw_frame): #FIX\n",
    "        \"\"\"Check if dialog region is white\"\"\"\n",
    "        # Extract region. Note hard coded values are for macs laptop, its a region of the screen where its all white on game over.\n",
    "        tlx = GAMEOVER_X - TOP_X\n",
    "        tly = GAMEOVER_Y - TOP_Y\n",
    "        w = GAMEOVER_W\n",
    "        h = GAMEOVER_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "        \n",
    "        # Check if white directly on BGR image\n",
    "        mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "        \n",
    "        # If all channels are ~255, mean will be ~255\n",
    "        return mean_val > 250\n",
    "\n",
    "    def runway_reward(self, raw_frame):\n",
    "        tlx = RUNWAY_X - TOP_X\n",
    "        tly = RUNWAY_Y - TOP_Y\n",
    "        w = RUNWAY_W\n",
    "        h = RUNWAY_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "        if len(roi.shape) == 3: #grayscale\n",
    "            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            roi_gray = roi\n",
    "        \n",
    "        platform_pixels = np.sum(roi_gray > 30)\n",
    "        total_pixels = roi_gray.size\n",
    "        platform_ratio = platform_pixels / total_pixels\n",
    "        \n",
    "        return platform_ratio * 0.25 #small reward based on % of runway occupied.\n",
    "\n",
    "    # -------------------------\n",
    "    # Take one step in environment \n",
    "    # -------------------------\n",
    "    def step(self, action): #NOT FINISHED DONT TOUCH\n",
    "        # step_start = time.time()\n",
    "        self._execute_action(action)\n",
    "\n",
    "        # Capture new frame\n",
    "        raw = self.capture_raw()\n",
    "\n",
    "        done = self.game_over(raw) #Boolean var\n",
    "\n",
    "        # Reward logic\n",
    "        if done:\n",
    "            reward = -50\n",
    "        else:\n",
    "            reward = 1 + self.runway_reward(raw) # small survival reward + a small alignment reward\n",
    "\n",
    "        # Preprocess\n",
    "        processed = preprocess_frame(raw)\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        \n",
    "        state = np.transpose(stacked, (1, 2, 0))  # (REOSLUTION,RESOLUTION,STACK_FRAMES)\n",
    "\n",
    "        # TARGET_TIMESTEP = 0.33 \n",
    "        # elapsed = time.time() - step_start\n",
    "        # if elapsed < TARGET_TIMESTEP:\n",
    "        #     time.sleep(TARGET_TIMESTEP - elapsed)\n",
    "\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _execute_action(self, action):\n",
    "        \"\"\"Execute action with proper hold durations\"\"\"\n",
    "        # Map actions to (key, duration_seconds)\n",
    "        action_config = {\n",
    "            0: (None, 0),           # No action\n",
    "            1: ('left', 0.05),       # Left short - 100ms\n",
    "            2: ('right', 0.05),      # Right short\n",
    "            3: ('up', 0.05),         # Up short (jump)\n",
    "            4: ('left', 0.1),      # Left medium - 250ms\n",
    "            5: ('right', 0.1),     # Right medium\n",
    "            6: ('up', 0.1),        # Up medium\n",
    "            7: ('left', 0.25),       # Left long - 500ms\n",
    "            8: ('right', 0.25),      # Right long\n",
    "            9: ('up', 0.25),         # Up long\n",
    "        }\n",
    "        \n",
    "        key, duration = action_config[action]\n",
    "        \n",
    "        if key is not None:\n",
    "            pyautogui.keyDown(key)\n",
    "            time.sleep(duration)\n",
    "            pyautogui.keyUp(key)\n",
    "        time.sleep(0.25-duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4956b50f-186c-4637-9921-97c2687ae8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #JUST FOR TESTING THINGS WORK WHEN RUNNING THE GAME.\n",
    "# sct = mss.mss()\n",
    "# monitor = {\n",
    "#     \"top\": TOP_Y,\n",
    "#     \"left\": TOP_X,\n",
    "#     \"width\": WIDTH,\n",
    "#     \"height\": HEIGHT\n",
    "# }\n",
    "# num_g_over = 0\n",
    "\n",
    "# def test_game_over(raw_frame):\n",
    "#     \"\"\"Check if dialog region is white\"\"\"\n",
    "#     # Extract region\n",
    "#     tlx = 860 - TOP_X\n",
    "#     tly = 435 - TOP_Y\n",
    "#     w = 70\n",
    "#     h = 45\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "    \n",
    "#     # Check if white directly on BGR image\n",
    "#     mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "    \n",
    "#     # If all channels are ~255, mean will be ~255\n",
    "#     return mean_val > 250\n",
    "\n",
    "# def test_runway_reward(raw_frame):\n",
    "#     tlx = RUNWAY_X - TOP_X\n",
    "#     tly = RUNWAY_Y - TOP_Y\n",
    "#     w = RUNWAY_W\n",
    "#     h = RUNWAY_H\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "#     if len(roi.shape) == 3: #grayscale\n",
    "#         roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         roi_gray = roid\n",
    "    \n",
    "#     platform_pixels = np.sum(roi_gray > 30)\n",
    "#     total_pixels = roi_gray.size\n",
    "#     platform_ratio = platform_pixels / total_pixels\n",
    "    \n",
    "#     return platform_ratio\n",
    "\n",
    "# while True:\n",
    "#     start_time = time.time()\n",
    "#     screenshot = np.array(sct.grab(monitor))\n",
    "#     img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "#     if test_game_over(img):\n",
    "#         num_g_over += 1\n",
    "#         print(f\"\\rgame over {num_g_over}\", end='', flush=True)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#     else:\n",
    "#         pass\n",
    "#         # print(test_runway_reward(img))\n",
    "#     elapsed = time.time() - start_time\n",
    "#     if elapsed < 1:\n",
    "#             time.sleep(1 - elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de65712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IHAVENT CHECKED THE CLASS BELOW YET THIS IS JUST CHAT SO DONT TREAT IT AS SOLIDLY IMPLEMENTED\n",
    "\n",
    "class PPOBuffer:\n",
    "    def __init__(self, size, obs_shape, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        size      : number of steps per rollout\n",
    "        obs_shape : shape of observation e.g. (RESOLUTION, RESOLUTION, FRAME_STACK)\n",
    "        gamma     : discount factor\n",
    "        lam       : GAE lambda\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "\n",
    "        # Allocate buffers\n",
    "        self.obs_buf = np.zeros((size, *obs_shape), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(size, dtype=np.int32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        # To be computed later\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0        # next index to write\n",
    "        self.path_start = 0 # start index of current trajectory\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Store one step of rollout data\n",
    "    # ---------------------------------------------------------\n",
    "    def store(self, obs, act, rew, done, val, logp):\n",
    "        assert self.ptr < self.size, \"PPOBuffer overflow!\"\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Finish trajectory and compute GAE + returns\n",
    "    # last_val is the value of the final observation (0 if done)\n",
    "    # ---------------------------------------------------------\n",
    "    def finish_trajectory(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Called at trajectory end or when episode completes.\n",
    "        Computes GAE advantage & discounted returns.\n",
    "        \"\"\"\n",
    "        i1 = self.path_start\n",
    "        i2 = self.ptr\n",
    "\n",
    "        rewards = np.append(self.rew_buf[i1:i2], last_val)\n",
    "        values  = np.append(self.val_buf[i1:i2], last_val)\n",
    "\n",
    "        # GAE-Lambda advantage calculation\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        adv = np.zeros_like(deltas)\n",
    "        last_gae = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            last_gae = deltas[t] + self.gamma * self.lam * last_gae * (1 - self.done_buf[i1 + t])\n",
    "            adv[t] = last_gae\n",
    "\n",
    "        self.adv_buf[i1:i2] = adv\n",
    "        self.ret_buf[i1:i2] = adv + self.val_buf[i1:i2]\n",
    "\n",
    "        self.path_start = self.ptr  # next trajectory starts here\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Retrieve all data, normalize adv, and reset pointer\n",
    "    # ---------------------------------------------------------\n",
    "    def prepare_for_training(self):\n",
    "        \"\"\"\n",
    "        Call this after all trajectories are collected and before get().\n",
    "        Normalizes advantages across the entire buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        # Normalize advantages\n",
    "        adv_mean = self.adv_buf.mean()\n",
    "        adv_std = self.adv_buf.std() + 1e-8\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "\n",
    "        \n",
    "    def get(self, batch_size=64):\n",
    "        \"\"\"Returns batches of rollout data.\"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        return {\n",
    "            'obs': self.obs_buf,\n",
    "            'act': self.act_buf,\n",
    "            'adv': self.adv_buf,\n",
    "            'ret': self.ret_buf,\n",
    "            'logp': self.logp_buf,\n",
    "            'val': self.val_buf\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset buffer for next rollout collection.\n",
    "        Call this after training is done for the epoch.\n",
    "        \"\"\"\n",
    "        self.ptr = 0\n",
    "        self.path_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcc7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning Rate: 0.0003\n",
      "  Steps per Epoch: 512\n",
      "  Total Epochs: 150\n",
      "  Gamma: 0.99, GAE Lambda: 0.95\n",
      "  Clip Ratio: 0.2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION AND HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "CLIP_RATIO = 0.2\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.015\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# Rollout parameters\n",
    "STEPS_PER_EPOCH = 512  # Number of steps per training epoch\n",
    "TRAIN_EPOCHS = 150     # Total number of epochs\n",
    "MINI_BATCH_SIZE = 64    # Size of mini-batches for SGD\n",
    "UPDATE_EPOCHS = 5      # Number of epochs to train on each batch\n",
    "\n",
    "# Discount and GAE\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "# Logging and checkpointing\n",
    "LOG_INTERVAL = 3          # Log every N epochs\n",
    "SAVE_INTERVAL = 25         # Save model every N epochs\n",
    "EVAL_EPISODES = 10          # Number of episodes for evaluation\n",
    "\n",
    "# Create directories for saving\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Steps per Epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"  Total Epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"  Gamma: {GAMMA}, GAE Lambda: {GAE_LAMBDA}\")\n",
    "print(f\"  Clip Ratio: {CLIP_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d9d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PPO TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_loss(model, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Compute PPO loss with clipping.\n",
    "    \n",
    "    Args:\n",
    "        model: PPOActorCritic model\n",
    "        obs: observations (batch_size, H, W, C)\n",
    "        actions: actions taken (batch_size,)\n",
    "        advantages: advantage estimates (batch_size,)\n",
    "        returns: discounted returns (batch_size,)\n",
    "        old_log_probs: old action log probabilities (batch_size,)\n",
    "        old_values: old value estimates (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, policy_loss, value_loss, entropy\n",
    "    \"\"\"\n",
    "    # Get current policy and value predictions\n",
    "    logits, values = model(obs, training=True)\n",
    "    values = tf.squeeze(values, axis=-1)\n",
    "    \n",
    "    # Compute log probabilities of actions\n",
    "    action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "    log_probs = action_dist.log_prob(actions)\n",
    "    \n",
    "    # Compute entropy for exploration bonus\n",
    "    entropy = tf.reduce_mean(action_dist.entropy())\n",
    "    \n",
    "    # Compute ratio for PPO\n",
    "    ratio = tf.exp(log_probs - old_log_probs)\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
    "    \n",
    "    # Policy loss with clipping\n",
    "    policy_loss_1 = -advantages * ratio\n",
    "    policy_loss_2 = -advantages * tf.clip_by_value(ratio, 1 - CLIP_RATIO, 1 + CLIP_RATIO)\n",
    "    policy_loss = tf.reduce_mean(tf.maximum(policy_loss_1, policy_loss_2))\n",
    "    \n",
    "    # Value loss with clipping\n",
    "    value_pred_clipped = old_values + tf.clip_by_value(\n",
    "        values - old_values, -CLIP_RATIO, CLIP_RATIO\n",
    "    )\n",
    "    value_loss_1 = tf.square(returns - values)\n",
    "    value_loss_2 = tf.square(returns - value_pred_clipped)\n",
    "    value_loss = 0.5 * tf.reduce_mean(tf.maximum(value_loss_1, value_loss_2))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Single training step with gradient computation.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss, policy_loss, value_loss, entropy = compute_loss(\n",
    "            model, obs, actions, advantages, returns, old_log_probs, old_values\n",
    "        )\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    \n",
    "    # Clip gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, MAX_GRAD_NORM)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb306cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout collection function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA COLLECTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def collect_rollout(env, model, buffer, num_steps):\n",
    "    \"\"\"\n",
    "    Collect trajectories by running the current policy in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        buffer: PPOBuffer instance\n",
    "        num_steps: number of steps to collect\n",
    "    \n",
    "    Returns:\n",
    "        ep_returns: list of episode returns\n",
    "        ep_lengths: list of episode lengths\n",
    "    \"\"\"\n",
    "    buffer.reset()  # Clear the buffer\n",
    "    \n",
    "    ep_returns = []\n",
    "    ep_lengths = []\n",
    "    current_ep_return = 0\n",
    "    current_ep_length = 0\n",
    "    \n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Get action from policy\n",
    "        # Get action from policy\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)  # Add batch dimension\n",
    "        logits, value = model(obs_tensor, training=False)\n",
    "\n",
    "# Sample action from distribution (use logits, not probs!)\n",
    "        action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "        action_tensor = action_dist.sample()[0]\n",
    "        action = action_tensor.numpy()\n",
    "        log_prob = action_dist.log_prob(action_tensor).numpy()\n",
    "        value = value.numpy()[0, 0]\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        buffer.store(obs, action, reward, done, value, log_prob)\n",
    "        \n",
    "        current_ep_return += reward\n",
    "        current_ep_length += 1\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            # Episode finished\n",
    "            buffer.finish_trajectory(0)  # Terminal state has value 0\n",
    "            ep_returns.append(current_ep_return)\n",
    "            ep_lengths.append(current_ep_length)\n",
    "            \n",
    "            # Reset for next episode\n",
    "            obs = env.reset()\n",
    "            current_ep_return = 0\n",
    "            current_ep_length = 0\n",
    "    \n",
    "    # If we ended mid-episode, bootstrap the value\n",
    "    if current_ep_length > 0:\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "        _, last_value = model(obs_tensor, training=False)\n",
    "        buffer.finish_trajectory(last_value.numpy()[0, 0])\n",
    "        ep_returns.append(current_ep_return)\n",
    "        ep_lengths.append(current_ep_length)\n",
    "    \n",
    "    buffer.prepare_for_training()\n",
    "\n",
    "    return ep_returns, ep_lengths\n",
    "\n",
    "\n",
    "print(\"Rollout collection function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8254b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_policy(env, model, num_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the current policy deterministically (greedy).\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        num_episodes: number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        mean_return: average episode return\n",
    "        std_return: standard deviation of returns\n",
    "        mean_length: average episode length\n",
    "    \"\"\"\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        ep_return = 0\n",
    "        ep_length = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Use greedy action (argmax)\n",
    "            obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "            logits, _ = model(obs_tensor, training=False)\n",
    "            action = tf.argmax(logits[0]).numpy()\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_return += reward\n",
    "            ep_length += 1\n",
    "            \n",
    "            # Safety: break if episode too long\n",
    "            if ep_length > 10000:\n",
    "                break\n",
    "        \n",
    "        episode_returns.append(ep_return)\n",
    "        episode_lengths.append(ep_length)\n",
    "    \n",
    "    return (\n",
    "        np.mean(episode_returns),\n",
    "        np.std(episode_returns),\n",
    "        np.mean(episode_lengths)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344df394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training components...\n",
      "Model initialized with 2,242,027 parameters\n",
      "\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Initializing training components...\")\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "# Training metrics\n",
    "training_stats = {\n",
    "    'epoch': [],\n",
    "    'mean_return': [],\n",
    "    'mean_length': [],\n",
    "    'policy_loss': [],\n",
    "    'value_loss': [],\n",
    "    'entropy': [],\n",
    "    'eval_return': [],\n",
    "    'eval_std': []\n",
    "}\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1bf418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q_/y2tlnq4d3gl5gqxkc15y__ym0000gn/T/ipykernel_48752/1679292621.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.logp_buf[self.ptr] = logp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/150 (350.4s)\n",
      "  Returns: -38.98 | Lengths: 9.7\n",
      "  Policy Loss: -0.0204\n",
      "  Value Loss: 520.0970\n",
      "  Entropy: 2.1839\n",
      "\n",
      "Epoch 6/150 (334.2s)\n",
      "  Returns: -37.47 | Lengths: 10.9\n",
      "  Policy Loss: -0.0039\n",
      "  Value Loss: 355.2604\n",
      "  Entropy: 2.0382\n",
      "\n",
      "Epoch 9/150 (344.3s)\n",
      "  Returns: -39.05 | Lengths: 9.7\n",
      "  Policy Loss: -0.0198\n",
      "  Value Loss: 358.7916\n",
      "  Entropy: 2.0517\n",
      "\n",
      "Epoch 12/150 (338.1s)\n",
      "  Returns: -38.06 | Lengths: 10.4\n",
      "  Policy Loss: -0.0221\n",
      "  Value Loss: 271.2982\n",
      "  Entropy: 2.0245\n",
      "\n",
      "Epoch 15/150 (344.6s)\n",
      "  Returns: -39.33 | Lengths: 9.5\n",
      "  Policy Loss: -0.0076\n",
      "  Value Loss: 250.0855\n",
      "  Entropy: 2.0146\n",
      "  [EVAL] Return: -40.97 ± 6.24 | Length: 8.7\n",
      "\n",
      "Epoch 18/150 (337.1s)\n",
      "  Returns: -38.52 | Lengths: 10.0\n",
      "  Policy Loss: -0.0062\n",
      "  Value Loss: 146.5814\n",
      "  Entropy: 1.8546\n",
      "\n",
      "Epoch 21/150 (314.4s)\n",
      "  Returns: -37.26 | Lengths: 11.9\n",
      "  Policy Loss: -0.0040\n",
      "  Value Loss: 89.5936\n",
      "  Entropy: 1.8358\n",
      "\n",
      "Epoch 24/150 (314.3s)\n",
      "  Returns: -37.93 | Lengths: 11.4\n",
      "  Policy Loss: -0.0097\n",
      "  Value Loss: 67.9985\n",
      "  Entropy: 1.5781\n",
      "  [SAVED] Checkpoint: checkpoints/model_epoch_25.h5\n",
      "\n",
      "Epoch 27/150 (326.2s)\n",
      "  Returns: -39.09 | Lengths: 9.7\n",
      "  Policy Loss: -0.0102\n",
      "  Value Loss: 53.8166\n",
      "  Entropy: 1.5460\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m epoch_start_time = time.time()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# === 1. Collect rollout data ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ep_returns, ep_lengths = \u001b[43mcollect_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m mean_return = np.mean(ep_returns) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ep_returns) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     12\u001b[39m mean_length = np.mean(ep_lengths) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ep_lengths) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mcollect_rollout\u001b[39m\u001b[34m(env, model, buffer, num_steps)\u001b[39m\n\u001b[32m     40\u001b[39m value = value.numpy()[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Take action in environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m next_obs, reward, done, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Store transition\u001b[39;00m\n\u001b[32m     46\u001b[39m buffer.store(obs, action, reward, done, value, log_prob)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mRun3Env.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action): \u001b[38;5;66;03m#NOT FINISHED DONT TOUCH\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# step_start = time.time()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# Capture new frame\u001b[39;00m\n\u001b[32m     86\u001b[39m     raw = \u001b[38;5;28mself\u001b[39m.capture_raw()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mRun3Env._execute_action\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     pyautogui.keyDown(key)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     time.sleep(duration)\n\u001b[32m    130\u001b[39m     pyautogui.keyUp(key)\n\u001b[32m    131\u001b[39m time.sleep(\u001b[32m0.25\u001b[39m-duration)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # === 1. Collect rollout data ===\n",
    "    ep_returns, ep_lengths = collect_rollout(env, model, buffer, STEPS_PER_EPOCH)\n",
    "    \n",
    "    mean_return = np.mean(ep_returns) if len(ep_returns) > 0 else 0\n",
    "    mean_length = np.mean(ep_lengths) if len(ep_lengths) > 0 else 0\n",
    "    \n",
    "    # === 2. Get training data from buffer ===\n",
    "    data = buffer.get()\n",
    "    obs_buf = data['obs']\n",
    "    act_buf = data['act']\n",
    "    adv_buf = data['adv']\n",
    "    ret_buf = data['ret']\n",
    "    logp_buf = data['logp']\n",
    "    val_buf = data['val']\n",
    "    \n",
    "    # === 3. Train on the collected data ===\n",
    "    total_losses = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    # Perform multiple epochs of training on the same batch\n",
    "    for i in range(UPDATE_EPOCHS):\n",
    "        # Shuffle indices\n",
    "        indices = np.arange(len(obs_buf))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Train on mini-batches\n",
    "        for start in range(0, len(obs_buf), MINI_BATCH_SIZE):\n",
    "            end = start + MINI_BATCH_SIZE\n",
    "            batch_idx = indices[start:end]\n",
    "            \n",
    "            batch_obs = tf.constant(obs_buf[batch_idx])\n",
    "            batch_act = tf.constant(act_buf[batch_idx])\n",
    "            batch_adv = tf.constant(adv_buf[batch_idx])\n",
    "            batch_ret = tf.constant(ret_buf[batch_idx])\n",
    "            batch_logp = tf.constant(logp_buf[batch_idx])\n",
    "            batch_val = tf.constant(val_buf[batch_idx])\n",
    "            \n",
    "            # Perform gradient update\n",
    "            total_loss, policy_loss, value_loss, entropy = train_step(\n",
    "                model, optimizer, batch_obs, batch_act,\n",
    "                batch_adv, batch_ret, batch_logp, batch_val\n",
    "            )\n",
    "            \n",
    "            total_losses.append(total_loss.numpy())\n",
    "            policy_losses.append(policy_loss.numpy())\n",
    "            value_losses.append(value_loss.numpy())\n",
    "            entropies.append(entropy.numpy())\n",
    "    \n",
    "    # === 4. Logging ===\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    if (epoch + 1) % LOG_INTERVAL == 0:\n",
    "        print(f\"\\nEpoch {epoch + 1}/{TRAIN_EPOCHS} ({epoch_time:.1f}s)\")\n",
    "        print(f\"  Returns: {mean_return:.2f} | Lengths: {mean_length:.1f}\")\n",
    "        print(f\"  Policy Loss: {np.mean(policy_losses):.4f}\")\n",
    "        print(f\"  Value Loss: {np.mean(value_losses):.4f}\")\n",
    "        print(f\"  Entropy: {np.mean(entropies):.4f}\")\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if (epoch + 1) % (LOG_INTERVAL * 5) == 0:\n",
    "            eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "            print(f\"  [EVAL] Return: {eval_return:.2f} ± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "            training_stats['eval_return'].append(eval_return)\n",
    "            training_stats['eval_std'].append(eval_std)\n",
    "    \n",
    "    # Store metrics\n",
    "    training_stats['epoch'].append(epoch + 1)\n",
    "    training_stats['mean_return'].append(mean_return)\n",
    "    training_stats['mean_length'].append(mean_length)\n",
    "    training_stats['policy_loss'].append(np.mean(policy_losses))\n",
    "    training_stats['value_loss'].append(np.mean(value_losses))\n",
    "    training_stats['entropy'].append(np.mean(entropies))\n",
    "    \n",
    "    # === 5. Save checkpoint ===\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        checkpoint_path = f'checkpoints/model_epoch_{epoch + 1}.h5'\n",
    "        model.save_weights(checkpoint_path)\n",
    "        print(f\"  [SAVED] Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2499d55-efb1-4ad2-bdbc-a662990bf2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST FOR LOADING AND EVALUATING A MODEL\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "model_path = 'checkpoints/model_epoch_75.h5'  # or whichever epoch you want\n",
    "model.load_weights(model_path)\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "print(f\"  [EVAL] Return: {eval_return:.2f} ± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "training_stats['eval_return'].append(eval_return)\n",
    "training_stats['eval_std'].append(eval_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78a597-272a-4951-8f32-aff17cf1cabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
