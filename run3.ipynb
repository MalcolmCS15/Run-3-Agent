{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb4b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful for Run3 project!\n",
      "TensorFlow version: 2.15.0\n",
      "NumPy version: 1.26.4\n",
      "OpenCV version: 4.10.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# -------- Image processing -----\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mss\n",
    "\n",
    "# -------- TensorFlow / Keras ----\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# -------- Misc / Debug ----------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"All imports successful for Run3 project!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e43e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish some variables and params for later\n",
    "\n",
    "#number of frames to feed model at a time. We input to the model the FRAME_STACK most recent frames\n",
    "FRAME_STACK = 4\n",
    "\n",
    "#variables for the location of the run 3 game on the screen. This is for Malcolms computer, if its diff for u make new vars\n",
    "TOP_X = 275\n",
    "TOP_Y = 195\n",
    "WIDTH = 725\n",
    "HEIGHT = 545\n",
    "\n",
    "GAMEOVER_X = 860\n",
    "GAMEOVER_Y = 435\n",
    "GAMEOVER_W = 70\n",
    "GAMEOVER_H = 45\n",
    "\n",
    "RUNWAY_X = 600\n",
    "RUNWAY_Y = 480\n",
    "RUNWAY_W = 135\n",
    "RUNWAY_H = 230\n",
    "\n",
    "#Which device is running the game. Add ur own if u wanna train. So we dont have to go all the way through everything and change\n",
    "MAC_LAPTOP = True \n",
    "MAC_MONITOR = False\n",
    "\n",
    "if MAC_LAPTOP: \n",
    "    TOP_X = 275\n",
    "    TOP_Y = 195\n",
    "    WIDTH = 725\n",
    "    HEIGHT = 545\n",
    "\n",
    "    GAMEOVER_X = 860\n",
    "    GAMEOVER_Y = 435\n",
    "    GAMEOVER_W = 70\n",
    "    GAMEOVER_H = 45\n",
    "\n",
    "    RUNWAY_X = 600\n",
    "    RUNWAY_Y = 480\n",
    "    RUNWAY_W = 135\n",
    "    RUNWAY_H = 230\n",
    "\n",
    "\n",
    "#resolution of the image were resizing to. This affects the input to our neural net directly.\n",
    "RESOLUTION = 96\n",
    "\n",
    "#number of actions the model can take. This is a super important thing to change if the model isnt training well. As of 12/5 were starting\n",
    "#with the model being able to take [no action, L_small, R_small, U_small, L_med, R_med ...etc.]\n",
    "NUM_ACTIONS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e191780a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_frame(image, resize=(RESOLUTION, RESOLUTION), gray=True):\n",
    "    \"\"\"\n",
    "    Captures a screenshot of the given region, converts to grayscale, resizes.\n",
    "    Returns numpy array of shape (resize[1], resize[0]).\n",
    "    \"\"\"\n",
    "    if gray:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize deterministically\n",
    "    small = cv2.resize(image, resize, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return small\n",
    "\n",
    "def stack_frames(frames, new_frame, stack_size=FRAME_STACK):\n",
    "    \"\"\"\n",
    "    Maintains a stack of frames to capture motion.\n",
    "    frames: deque of previous frames\n",
    "    new_frame: newest preprocessed frame\n",
    "    Returns stack of frames\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        # Initialize with repeated frame\n",
    "        for _ in range(stack_size):\n",
    "            frames.append(new_frame)\n",
    "    else:\n",
    "        frames.append(new_frame)\n",
    "        if len(frames) > stack_size:\n",
    "            frames.popleft()\n",
    "    return np.stack(frames, axis=0)\n",
    "\n",
    "#TEST to see if screen grab is working\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "sct = mss.mss()\n",
    "monitor = {\n",
    "    \"top\": TOP_Y,\n",
    "    \"left\": TOP_X,\n",
    "    \"width\": WIDTH,\n",
    "    \"height\": HEIGHT\n",
    "}\n",
    "screenshot = np.array(sct.grab(monitor))\n",
    "img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_BGRA2BGR)\n",
    "processed = preprocess_frame(img)\n",
    "plt.imshow(processed, cmap=\"gray\")\n",
    "plt.title(\"Preprocessed frame (grayscale + resized)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b158ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorCritic(tf.keras.Model):\n",
    "    def __init__(self, input_channels=FRAME_STACK, num_actions=NUM_ACTIONS):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "\n",
    "        # TensorFlow expects channels-last â†’ (RESOLUTION, RESOLUTION, C)\n",
    "        self.input_channels = input_channels\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # ---------- CNN Backbone ----------\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=4, strides=2, activation='relu')\n",
    "        self.conv3 = layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')\n",
    "\n",
    "        #max pool? think about max pool if we use a larger resolution. But these convs also scale down.\n",
    "\n",
    "        # compute flatten size\n",
    "        self._conv_out_size = self._get_conv_out((RESOLUTION, RESOLUTION, input_channels))\n",
    "\n",
    "        # ---------- Shared Fully Connected ----------\n",
    "        self.fc = layers.Dense(512, activation='relu')\n",
    "\n",
    "        # ---------- Actor Head ----------\n",
    "        self.actor_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.actor_logits = layers.Dense(num_actions, activation=None)\n",
    "\n",
    "        # ---------- Critic Head ----------\n",
    "        self.critic_fc1 = layers.Dense(64, activation='relu')\n",
    "        self.critic_value = layers.Dense(1, activation=None)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Compute conv output size by running dummy tensor. This saves us work if we change the CNN structure\n",
    "    # -------------------------------------------------\n",
    "    def _get_conv_out(self, shape):\n",
    "        dummy = tf.zeros((1, *shape), dtype=tf.float32)\n",
    "        x = self.conv1(dummy)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.shape[1:]))\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Forward pass\n",
    "    # -------------------------------------------------\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x expected as (batch, RESOLUTION, RESOLUTION, 4)\n",
    "        \"\"\"\n",
    "        x = tf.cast(x, tf.float32) / 255.0 #normalize to [0,1]\n",
    "\n",
    "        # CNN backbone\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = tf.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "        # Shared FC\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # ---- Actor ----\n",
    "        a = self.actor_fc1(x)\n",
    "        logits = self.actor_logits(a)\n",
    "\n",
    "        # ---- Critic ----\n",
    "        c = self.critic_fc1(x)\n",
    "        value = self.critic_value(c)\n",
    "\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "026a525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run3Env:\n",
    "    def __init__(self, region=(TOP_X, TOP_Y, WIDTH, HEIGHT), frame_stack=FRAME_STACK):\n",
    "        self.region = region\n",
    "        self.frame_stack = frame_stack #integer, not the actual stack. Do we need instance var?\n",
    "        self.frames = deque(maxlen=frame_stack)\n",
    "\n",
    "        self.sct = mss.mss()\n",
    "        self.monitor = {\n",
    "            \"top\": region[1],      # TOP_Y\n",
    "            \"left\": region[0],     # TOP_X\n",
    "            \"width\": region[2],    # WIDTH\n",
    "            \"height\": region[3]    # HEIGHT\n",
    "        }\n",
    "\n",
    "    # -------------------------\n",
    "    # Reset environment\n",
    "    # -------------------------\n",
    "    def reset(self):\n",
    "        # Click to restart game. 900,650 is just off the screen a bit, click twice to bypass the \"continue\" and \"score\". \n",
    "        #we can also press a button to make it better for everyones computer\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        time.sleep(.7)\n",
    "        pyautogui.click(900, 650)\n",
    "        self.frames.clear()\n",
    "\n",
    "        # Get initial observation\n",
    "        raw = self.capture_raw()\n",
    "        processed = preprocess_frame(raw) #function defined at the top\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        return np.transpose(stacked, (1, 2, 0))  # (RESOLUTION, RESOLUTION, FRAME_STACK). \n",
    "\n",
    "    # -------------------------\n",
    "    # Capture raw screenshot\n",
    "    # -------------------------\n",
    "    def capture_raw(self):\n",
    "        screenshot = np.array(self.sct.grab(self.monitor))\n",
    "        # mss returns BGRA, convert to BGR. Also mss is much faster than pyautogui so we use it for more fps.\n",
    "        img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "        return img\n",
    "\n",
    "    # -------------------------\n",
    "    # Detect game over\n",
    "    # -------------------------\n",
    "    def game_over(self, raw_frame): #FIX\n",
    "        \"\"\"Check if dialog region is white\"\"\"\n",
    "        # Extract region. Note hard coded values are for macs laptop, its a region of the screen where its all white on game over.\n",
    "        tlx = GAMEOVER_X - TOP_X\n",
    "        tly = GAMEOVER_Y - TOP_Y\n",
    "        w = GAMEOVER_W\n",
    "        h = GAMEOVER_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "        \n",
    "        # Check if white directly on BGR image\n",
    "        mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "        \n",
    "        # If all channels are ~255, mean will be ~255\n",
    "        return mean_val > 250\n",
    "\n",
    "    def runway_reward(self, raw_frame):\n",
    "        tlx = RUNWAY_X - TOP_X\n",
    "        tly = RUNWAY_Y - TOP_Y\n",
    "        w = RUNWAY_W\n",
    "        h = RUNWAY_H\n",
    "        roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "        if len(roi.shape) == 3: #grayscale\n",
    "            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            roi_gray = roi\n",
    "        \n",
    "        platform_pixels = np.sum(roi_gray > 30)\n",
    "        total_pixels = roi_gray.size\n",
    "        platform_ratio = platform_pixels / total_pixels\n",
    "        \n",
    "        return platform_ratio * 0.25 #small reward based on % of runway occupied.\n",
    "\n",
    "    # -------------------------\n",
    "    # Take one step in environment \n",
    "    # -------------------------\n",
    "    def step(self, action): #NOT FINISHED DONT TOUCH\n",
    "        # step_start = time.time()\n",
    "        self._execute_action(action)\n",
    "\n",
    "        # Capture new frame\n",
    "        raw = self.capture_raw()\n",
    "\n",
    "        done = self.game_over(raw) #Boolean var\n",
    "\n",
    "        # Reward logic\n",
    "        if done:\n",
    "            reward = -50\n",
    "        else:\n",
    "            reward = 1 + self.runway_reward(raw) # small survival reward + a small alignment reward\n",
    "\n",
    "        # Preprocess\n",
    "        processed = preprocess_frame(raw)\n",
    "        stacked = stack_frames(self.frames, processed)\n",
    "        \n",
    "        state = np.transpose(stacked, (1, 2, 0))  # (REOSLUTION,RESOLUTION,STACK_FRAMES)\n",
    "\n",
    "        # TARGET_TIMESTEP = 0.33 \n",
    "        # elapsed = time.time() - step_start\n",
    "        # if elapsed < TARGET_TIMESTEP:\n",
    "        #     time.sleep(TARGET_TIMESTEP - elapsed)\n",
    "\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _execute_action(self, action):\n",
    "        \"\"\"Execute action with proper hold durations\"\"\"\n",
    "        # Map actions to (key, duration_seconds)\n",
    "        action_config = {\n",
    "            0: (None, 0),           # No action\n",
    "            1: ('left', 0.05),       # Left short - 100ms\n",
    "            2: ('right', 0.05),      # Right short\n",
    "            3: ('up', 0.05),         # Up short (jump)\n",
    "            4: ('left', 0.1),      # Left medium - 250ms\n",
    "            5: ('right', 0.1),     # Right medium\n",
    "            6: ('up', 0.1),        # Up medium\n",
    "            7: ('left', 0.25),       # Left long - 500ms\n",
    "            8: ('right', 0.25),      # Right long\n",
    "            9: ('up', 0.25),         # Up long\n",
    "        }\n",
    "        \n",
    "        key, duration = action_config[action]\n",
    "        \n",
    "        if key is not None:\n",
    "            pyautogui.keyDown(key)\n",
    "            time.sleep(duration)\n",
    "            pyautogui.keyUp(key)\n",
    "        time.sleep(0.25-duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4956b50f-186c-4637-9921-97c2687ae8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #JUST FOR TESTING THINGS WORK WHEN RUNNING THE GAME.\n",
    "# sct = mss.mss()\n",
    "# monitor = {\n",
    "#     \"top\": TOP_Y,\n",
    "#     \"left\": TOP_X,\n",
    "#     \"width\": WIDTH,\n",
    "#     \"height\": HEIGHT\n",
    "# }\n",
    "# num_g_over = 0\n",
    "\n",
    "# def test_game_over(raw_frame):\n",
    "#     \"\"\"Check if dialog region is white\"\"\"\n",
    "#     # Extract region\n",
    "#     tlx = 860 - TOP_X\n",
    "#     tly = 435 - TOP_Y\n",
    "#     w = 70\n",
    "#     h = 45\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "    \n",
    "#     # Check if white directly on BGR image\n",
    "#     mean_val = roi.mean()  # Average across all pixels AND all channels\n",
    "    \n",
    "#     # If all channels are ~255, mean will be ~255\n",
    "#     return mean_val > 250\n",
    "\n",
    "# def test_runway_reward(raw_frame):\n",
    "#     tlx = RUNWAY_X - TOP_X\n",
    "#     tly = RUNWAY_Y - TOP_Y\n",
    "#     w = RUNWAY_W\n",
    "#     h = RUNWAY_H\n",
    "#     roi = raw_frame[tly:tly+h, tlx:tlx+w]\n",
    "\n",
    "#     if len(roi.shape) == 3: #grayscale\n",
    "#         roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         roi_gray = roid\n",
    "    \n",
    "#     platform_pixels = np.sum(roi_gray > 30)\n",
    "#     total_pixels = roi_gray.size\n",
    "#     platform_ratio = platform_pixels / total_pixels\n",
    "    \n",
    "#     return platform_ratio\n",
    "\n",
    "# while True:\n",
    "#     start_time = time.time()\n",
    "#     screenshot = np.array(sct.grab(monitor))\n",
    "#     img = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2BGR)\n",
    "#     if test_game_over(img):\n",
    "#         num_g_over += 1\n",
    "#         print(f\"\\rgame over {num_g_over}\", end='', flush=True)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#         time.sleep(0.7)\n",
    "#         pyautogui.click(900, 650)\n",
    "#     else:\n",
    "#         pass\n",
    "#         # print(test_runway_reward(img))\n",
    "#     elapsed = time.time() - start_time\n",
    "#     if elapsed < 1:\n",
    "#             time.sleep(1 - elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de65712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IHAVENT CHECKED THE CLASS BELOW YET THIS IS JUST CHAT SO DONT TREAT IT AS SOLIDLY IMPLEMENTED\n",
    "\n",
    "class PPOBuffer:\n",
    "    def __init__(self, size, obs_shape, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        size      : number of steps per rollout\n",
    "        obs_shape : shape of observation e.g. (RESOLUTION, RESOLUTION, FRAME_STACK)\n",
    "        gamma     : discount factor\n",
    "        lam       : GAE lambda\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "\n",
    "        # Allocate buffers\n",
    "        self.obs_buf = np.zeros((size, *obs_shape), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(size, dtype=np.int32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        # To be computed later\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0        # next index to write\n",
    "        self.path_start = 0 # start index of current trajectory\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Store one step of rollout data\n",
    "    # ---------------------------------------------------------\n",
    "    def store(self, obs, act, rew, done, val, logp):\n",
    "        assert self.ptr < self.size, \"PPOBuffer overflow!\"\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Finish trajectory and compute GAE + returns\n",
    "    # last_val is the value of the final observation (0 if done)\n",
    "    # ---------------------------------------------------------\n",
    "    def finish_trajectory(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Called at trajectory end or when episode completes.\n",
    "        Computes GAE advantage & discounted returns.\n",
    "        \"\"\"\n",
    "        i1 = self.path_start\n",
    "        i2 = self.ptr\n",
    "\n",
    "        rewards = np.append(self.rew_buf[i1:i2], last_val)\n",
    "        values  = np.append(self.val_buf[i1:i2], last_val)\n",
    "\n",
    "        # GAE-Lambda advantage calculation\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        adv = np.zeros_like(deltas)\n",
    "        last_gae = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            last_gae = deltas[t] + self.gamma * self.lam * last_gae * (1 - self.done_buf[i1 + t])\n",
    "            adv[t] = last_gae\n",
    "\n",
    "        self.adv_buf[i1:i2] = adv\n",
    "        self.ret_buf[i1:i2] = adv + self.val_buf[i1:i2]\n",
    "\n",
    "        self.path_start = self.ptr  # next trajectory starts here\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Retrieve all data, normalize adv, and reset pointer\n",
    "    # ---------------------------------------------------------\n",
    "    def prepare_for_training(self):\n",
    "        \"\"\"\n",
    "        Call this after all trajectories are collected and before get().\n",
    "        Normalizes advantages across the entire buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        # Normalize advantages\n",
    "        adv_mean = self.adv_buf.mean()\n",
    "        adv_std = self.adv_buf.std() + 1e-8\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "\n",
    "        \n",
    "    def get(self, batch_size=64):\n",
    "        \"\"\"Returns batches of rollout data.\"\"\"\n",
    "        assert self.ptr == self.size, \"Buffer not full!\"\n",
    "    \n",
    "        return {\n",
    "            'obs': self.obs_buf,\n",
    "            'act': self.act_buf,\n",
    "            'adv': self.adv_buf,\n",
    "            'ret': self.ret_buf,\n",
    "            'logp': self.logp_buf,\n",
    "            'val': self.val_buf\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset buffer for next rollout collection.\n",
    "        Call this after training is done for the epoch.\n",
    "        \"\"\"\n",
    "        self.ptr = 0\n",
    "        self.path_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning Rate: 0.001\n",
      "  Steps per Epoch: 512\n",
      "  Total Epochs: 75\n",
      "  Gamma: 0.99, GAE Lambda: 0.95\n",
      "  Clip Ratio: 0.3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION AND HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "CLIP_RATIO = 0.2\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.015\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# Rollout parameters\n",
    "STEPS_PER_EPOCH = 512  # Number of steps per training epoch\n",
    "TRAIN_EPOCHS = 150     # Total number of epochs\n",
    "MINI_BATCH_SIZE = 32    # Size of mini-batches for SGD\n",
    "UPDATE_EPOCHS = 20      # Number of epochs to train on each batch\n",
    "\n",
    "# Discount and GAE\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "# Logging and checkpointing\n",
    "LOG_INTERVAL = 3          # Log every N epochs\n",
    "SAVE_INTERVAL = 25         # Save model every N epochs\n",
    "EVAL_EPISODES = 10          # Number of episodes for evaluation\n",
    "\n",
    "# Create directories for saving\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Steps per Epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"  Total Epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"  Gamma: {GAMMA}, GAE Lambda: {GAE_LAMBDA}\")\n",
    "print(f\"  Clip Ratio: {CLIP_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d9d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PPO TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_loss(model, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Compute PPO loss with clipping.\n",
    "    \n",
    "    Args:\n",
    "        model: PPOActorCritic model\n",
    "        obs: observations (batch_size, H, W, C)\n",
    "        actions: actions taken (batch_size,)\n",
    "        advantages: advantage estimates (batch_size,)\n",
    "        returns: discounted returns (batch_size,)\n",
    "        old_log_probs: old action log probabilities (batch_size,)\n",
    "        old_values: old value estimates (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, policy_loss, value_loss, entropy\n",
    "    \"\"\"\n",
    "    # Get current policy and value predictions\n",
    "    logits, values = model(obs, training=True)\n",
    "    values = tf.squeeze(values, axis=-1)\n",
    "    \n",
    "    # Compute log probabilities of actions\n",
    "    action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "    log_probs = action_dist.log_prob(actions)\n",
    "    \n",
    "    # Compute entropy for exploration bonus\n",
    "    entropy = tf.reduce_mean(action_dist.entropy())\n",
    "    \n",
    "    # Compute ratio for PPO\n",
    "    ratio = tf.exp(log_probs - old_log_probs)\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
    "    \n",
    "    # Policy loss with clipping\n",
    "    policy_loss_1 = -advantages * ratio\n",
    "    policy_loss_2 = -advantages * tf.clip_by_value(ratio, 1 - CLIP_RATIO, 1 + CLIP_RATIO)\n",
    "    policy_loss = tf.reduce_mean(tf.maximum(policy_loss_1, policy_loss_2))\n",
    "    \n",
    "    # Value loss with clipping\n",
    "    value_pred_clipped = old_values + tf.clip_by_value(\n",
    "        values - old_values, -CLIP_RATIO, CLIP_RATIO\n",
    "    )\n",
    "    value_loss_1 = tf.square(returns - values)\n",
    "    value_loss_2 = tf.square(returns - value_pred_clipped)\n",
    "    value_loss = 0.5 * tf.reduce_mean(tf.maximum(value_loss_1, value_loss_2))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, obs, actions, advantages, returns, old_log_probs, old_values):\n",
    "    \"\"\"\n",
    "    Single training step with gradient computation.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss, policy_loss, value_loss, entropy = compute_loss(\n",
    "            model, obs, actions, advantages, returns, old_log_probs, old_values\n",
    "        )\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    \n",
    "    # Clip gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, MAX_GRAD_NORM)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb306cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout collection function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA COLLECTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def collect_rollout(env, model, buffer, num_steps):\n",
    "    \"\"\"\n",
    "    Collect trajectories by running the current policy in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        buffer: PPOBuffer instance\n",
    "        num_steps: number of steps to collect\n",
    "    \n",
    "    Returns:\n",
    "        ep_returns: list of episode returns\n",
    "        ep_lengths: list of episode lengths\n",
    "    \"\"\"\n",
    "    buffer.reset()  # Clear the buffer\n",
    "    \n",
    "    ep_returns = []\n",
    "    ep_lengths = []\n",
    "    current_ep_return = 0\n",
    "    current_ep_length = 0\n",
    "    \n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Get action from policy\n",
    "        # Get action from policy\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)  # Add batch dimension\n",
    "        logits, value = model(obs_tensor, training=False)\n",
    "\n",
    "# Sample action from distribution (use logits, not probs!)\n",
    "        action_dist = tf.compat.v1.distributions.Categorical(logits=logits)\n",
    "        action_tensor = action_dist.sample()[0]\n",
    "        action = action_tensor.numpy()\n",
    "        log_prob = action_dist.log_prob(action_tensor).numpy()\n",
    "        value = value.numpy()[0, 0]\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        buffer.store(obs, action, reward, done, value, log_prob)\n",
    "        \n",
    "        current_ep_return += reward\n",
    "        current_ep_length += 1\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            # Episode finished\n",
    "            buffer.finish_trajectory(0)  # Terminal state has value 0\n",
    "            ep_returns.append(current_ep_return)\n",
    "            ep_lengths.append(current_ep_length)\n",
    "            \n",
    "            # Reset for next episode\n",
    "            obs = env.reset()\n",
    "            current_ep_return = 0\n",
    "            current_ep_length = 0\n",
    "    \n",
    "    # If we ended mid-episode, bootstrap the value\n",
    "    if current_ep_length > 0:\n",
    "        obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "        _, last_value = model(obs_tensor, training=False)\n",
    "        buffer.finish_trajectory(last_value.numpy()[0, 0])\n",
    "        ep_returns.append(current_ep_return)\n",
    "        ep_lengths.append(current_ep_length)\n",
    "    \n",
    "    buffer.prepare_for_training()\n",
    "\n",
    "    return ep_returns, ep_lengths\n",
    "\n",
    "\n",
    "print(\"Rollout collection function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b8254b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_policy(env, model, num_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the current policy deterministically (greedy).\n",
    "    \n",
    "    Args:\n",
    "        env: Run3Env instance\n",
    "        model: PPOActorCritic model\n",
    "        num_episodes: number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        mean_return: average episode return\n",
    "        std_return: standard deviation of returns\n",
    "        mean_length: average episode length\n",
    "    \"\"\"\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        ep_return = 0\n",
    "        ep_length = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Use greedy action (argmax)\n",
    "            obs_tensor = tf.expand_dims(obs, axis=0)\n",
    "            logits, _ = model(obs_tensor, training=False)\n",
    "            action = tf.argmax(logits[0]).numpy()\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_return += reward\n",
    "            ep_length += 1\n",
    "            \n",
    "            # Safety: break if episode too long\n",
    "            if ep_length > 10000:\n",
    "                break\n",
    "        \n",
    "        episode_returns.append(ep_return)\n",
    "        episode_lengths.append(ep_length)\n",
    "    \n",
    "    return (\n",
    "        np.mean(episode_returns),\n",
    "        np.std(episode_returns),\n",
    "        np.mean(episode_lengths)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344df394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training components...\n",
      "Model initialized with 4,863,467 parameters\n",
      "\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Initializing training components...\")\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "# Training metrics\n",
    "training_stats = {\n",
    "    'epoch': [],\n",
    "    'mean_return': [],\n",
    "    'mean_length': [],\n",
    "    'policy_loss': [],\n",
    "    'value_loss': [],\n",
    "    'entropy': [],\n",
    "    'eval_return': [],\n",
    "    'eval_std': []\n",
    "}\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c1bf418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/q_/y2tlnq4d3gl5gqxkc15y__ym0000gn/T/ipykernel_41226/1310795699.py:36: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/q_/y2tlnq4d3gl5gqxkc15y__ym0000gn/T/ipykernel_41226/1310795699.py:36: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/ops/distributions/categorical.py:230: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/ops/distributions/categorical.py:230: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elybrayboy/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "/var/folders/q_/y2tlnq4d3gl5gqxkc15y__ym0000gn/T/ipykernel_41226/1679292621.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.logp_buf[self.ptr] = logp\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m batch_val = tf.constant(val_buf[batch_idx])\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Perform gradient update\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m total_loss, policy_loss, value_loss, entropy = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_ret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_logp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_val\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m total_losses.append(total_loss.numpy())\n\u001b[32m     54\u001b[39m policy_losses.append(policy_loss.numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    829\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    835\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    865\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    866\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    872\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1319\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1321\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1322\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1324\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1325\u001b[39m     args,\n\u001b[32m   1326\u001b[39m     possible_gradient_type,\n\u001b[32m   1327\u001b[39m     executing_eagerly)\n\u001b[32m   1328\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1484\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1494\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1495\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1496\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1500\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1501\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/csci1470-course/Run-3-Agent/run3_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # === 1. Collect rollout data ===\n",
    "    ep_returns, ep_lengths = collect_rollout(env, model, buffer, STEPS_PER_EPOCH)\n",
    "    \n",
    "    mean_return = np.mean(ep_returns) if len(ep_returns) > 0 else 0\n",
    "    mean_length = np.mean(ep_lengths) if len(ep_lengths) > 0 else 0\n",
    "    \n",
    "    # === 2. Get training data from buffer ===\n",
    "    data = buffer.get()\n",
    "    obs_buf = data['obs']\n",
    "    act_buf = data['act']\n",
    "    adv_buf = data['adv']\n",
    "    ret_buf = data['ret']\n",
    "    logp_buf = data['logp']\n",
    "    val_buf = data['val']\n",
    "    \n",
    "    # === 3. Train on the collected data ===\n",
    "    total_losses = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    # Perform multiple epochs of training on the same batch\n",
    "    for i in range(UPDATE_EPOCHS):\n",
    "        # Shuffle indices\n",
    "        indices = np.arange(len(obs_buf))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Train on mini-batches\n",
    "        for start in range(0, len(obs_buf), MINI_BATCH_SIZE):\n",
    "            end = start + MINI_BATCH_SIZE\n",
    "            batch_idx = indices[start:end]\n",
    "            \n",
    "            batch_obs = tf.constant(obs_buf[batch_idx])\n",
    "            batch_act = tf.constant(act_buf[batch_idx])\n",
    "            batch_adv = tf.constant(adv_buf[batch_idx])\n",
    "            batch_ret = tf.constant(ret_buf[batch_idx])\n",
    "            batch_logp = tf.constant(logp_buf[batch_idx])\n",
    "            batch_val = tf.constant(val_buf[batch_idx])\n",
    "            \n",
    "            # Perform gradient update\n",
    "            total_loss, policy_loss, value_loss, entropy = train_step(\n",
    "                model, optimizer, batch_obs, batch_act,\n",
    "                batch_adv, batch_ret, batch_logp, batch_val\n",
    "            )\n",
    "            \n",
    "            total_losses.append(total_loss.numpy())\n",
    "            policy_losses.append(policy_loss.numpy())\n",
    "            value_losses.append(value_loss.numpy())\n",
    "            entropies.append(entropy.numpy())\n",
    "    \n",
    "    # === 4. Logging ===\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    if (epoch + 1) % LOG_INTERVAL == 0:\n",
    "        print(f\"\\nEpoch {epoch + 1}/{TRAIN_EPOCHS} ({epoch_time:.1f}s)\")\n",
    "        print(f\"  Returns: {mean_return:.2f} | Lengths: {mean_length:.1f}\")\n",
    "        print(f\"  Policy Loss: {np.mean(policy_losses):.4f}\")\n",
    "        print(f\"  Value Loss: {np.mean(value_losses):.4f}\")\n",
    "        print(f\"  Entropy: {np.mean(entropies):.4f}\")\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if (epoch + 1) % (LOG_INTERVAL * 5) == 0:\n",
    "            eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "            print(f\"  [EVAL] Return: {eval_return:.2f} Â± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "            training_stats['eval_return'].append(eval_return)\n",
    "            training_stats['eval_std'].append(eval_std)\n",
    "    \n",
    "    # Store metrics\n",
    "    training_stats['epoch'].append(epoch + 1)\n",
    "    training_stats['mean_return'].append(mean_return)\n",
    "    training_stats['mean_length'].append(mean_length)\n",
    "    training_stats['policy_loss'].append(np.mean(policy_losses))\n",
    "    training_stats['value_loss'].append(np.mean(value_losses))\n",
    "    training_stats['entropy'].append(np.mean(entropies))\n",
    "    \n",
    "    # === 5. Save checkpoint ===\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        checkpoint_path = f'checkpoints/model_epoch_{epoch + 1}.h5'\n",
    "        model.save_weights(checkpoint_path)\n",
    "        print(f\"  [SAVED] Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2499d55-efb1-4ad2-bdbc-a662990bf2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 4,863,467 parameters\n",
      "Loaded model from checkpoints/model_epoch_75.h5\n",
      "  [EVAL] Return: -36.89 Â± 6.77 | Length: 12.2\n"
     ]
    }
   ],
   "source": [
    "#JUST FOR LOADING AND EVALUATING A MODEL\n",
    "\n",
    "env = Run3Env()\n",
    "model = PPOActorCritic(input_channels=FRAME_STACK, num_actions=NUM_ACTIONS)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "buffer = PPOBuffer(\n",
    "    size=STEPS_PER_EPOCH,\n",
    "    obs_shape=(RESOLUTION, RESOLUTION, FRAME_STACK),\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA\n",
    ")\n",
    "\n",
    "# Build model by running a forward pass\n",
    "dummy_obs = tf.random.normal((1, RESOLUTION, RESOLUTION, FRAME_STACK))\n",
    "_ = model(dummy_obs)\n",
    "print(f\"Model initialized with {model.count_params():,} parameters\")\n",
    "\n",
    "model_path = 'checkpoints/model_epoch_75.h5'  # or whichever epoch you want\n",
    "model.load_weights(model_path)\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "eval_return, eval_std, eval_length = evaluate_policy(env, model, EVAL_EPISODES)\n",
    "print(f\"  [EVAL] Return: {eval_return:.2f} Â± {eval_std:.2f} | Length: {eval_length:.1f}\")\n",
    "training_stats['eval_return'].append(eval_return)\n",
    "training_stats['eval_std'].append(eval_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78a597-272a-4951-8f32-aff17cf1cabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
